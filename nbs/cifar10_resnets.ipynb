{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10_resnets.ipynb","provenance":[],"collapsed_sections":["ifJ7hMp3d3YX","jw1x7KRVYVVp","pYMCWMWSbPhV","kfDTqx65eGz_","NcEQSK1pbCyD","UW-Kq4s0sCj8","aKTbrfjVUMCM"],"toc_visible":true,"authorship_tag":"ABX9TyOwicmsWTsZqDV8gWGzCdpC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vdg2YycmB-JN","colab_type":"code","colab":{}},"source":["# ------------------ check allocated GPU --------------------\n","\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","\n","# XXX: only one GPU on Colab isnâ€™t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","  process = psutil.Process(os.getpid())\n","  print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n","  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","\n","printm()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifJ7hMp3d3YX","colab_type":"text"},"source":["# IMPORTS #"]},{"cell_type":"code","metadata":{"id":"37tSkrbPCKXO","colab_type":"code","colab":{}},"source":["# import the necessary packages\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import GlobalMaxPooling2D\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import multiply\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Permute\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import add\n","\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras.regularizers import l2, Regularizer\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras import backend as K\n","from tensorflow import keras\n","\n","from tensorflow.keras.losses import CategoricalCrossentropy, sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import SGD\n","\n","from sklearn.feature_extraction.image import extract_patches_2d\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","import tempfile\n","import json\n","import time\n","import cv2\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mE8t_m3EOJ8a","colab_type":"code","colab":{}},"source":["! mkdir models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pLyYvMtGB1uX","colab_type":"text"},"source":["# CUSTOM REGULARIZER #"]},{"cell_type":"code","metadata":{"id":"n8oE6BJnB58V","colab_type":"code","colab":{}},"source":["class SimilarityPenalizer(Regularizer):\n","    def __init__(self, lamb, l2_reg):\n","        super(SimilarityPenalizer, self).__init__()\n","        self.lamb = lamb\n","        self.l2_reg = l2_reg\n","    \n","    def __call__(self, x):\n","        # x_copy = x\n","\n","        # x = tf.reshape(x, (-1, x.shape[-1]))\n","        # mu = tf.math.reduce_mean(x, axis = 0, keepdims = True)\n","\n","        # x_ = x - mu\n","        # x_ = tf.reshape(x_, (x_.shape[0], x_.shape[1], 1))\n","        # x_ = tf.repeat(x_, x_.shape[1], axis = -1)\n","\n","        # C  = (1 / x_.shape[0]) * tf.reduce_sum(tf.multiply(x_, tf.transpose(x_, perm = (0, 2, 1))), axis = 0)\n","\n","        # dist = (1 / 2) * (tf.norm(C) - tf.norm(tf.linalg.diag_part(C)))\n","\n","        # return ((self.lamb * dist) + (self.l2_reg * tf.math.reduce_sum(tf.math.square(x_copy))))\n","\n","        x_copy = x\n","        \n","        x = tf.reshape(x, (x.shape[-1], -1))\n","        x = x / (tf.linalg.norm(x, axis = -1, keepdims = True))\n","        cs = tf.matmul(x, tf.transpose(x))\n","        cs = cs * (1.0 - tf.eye(cs.shape[0], dtype = cs.dtype))\n","\n","        dist = tf.reduce_sum(tf.math.log(1.0 + tf.exp(10.0 * (cs - 1.0))))\n","\n","        return (tf.cast((self.lamb * dist), tf.float32) + (self.l2_reg * tf.math.reduce_sum(tf.math.square(x_copy))))\n","    \n","    def get_config(self):\n","        return {\"lamb\" : float(self.lamb),\n","                \"l2_reg\" : float(self.l2_reg)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_G2OfC_1ML_","colab_type":"code","colab":{}},"source":["class ActivityRegularizer(Regularizer):\n","    def __init__(self, lamb):\n","        super(ActivityRegularizer, self).__init__()\n","        self.lamb = lamb\n","    \n","    def __call__(self, x):\n","        x_ = tf.reduce_mean(x, axis = (1, 2))\n","        x_ = tf.reshape(x_, (-1, x_.shape[1], 1))\n","        x_ = tf.repeat(x_, x_.shape[-2], axis = -1)\n","\n","        dist = tf.reduce_mean(tf.abs(tf.subtract(x_, tf.transpose(x_, perm = (0, 2, 1)))))\n","\n","        return (self.lamb / dist)\n","\n","    def get_config(self):\n","        return {\"lamb\" : self.lamb}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAdFB47T_ZsG","colab_type":"code","colab":{}},"source":["class NegReg(Regularizer):\n","    def __init__(self, reg, eta, l2_reg):\n","        super(NegReg, self).__init__()\n","        self.reg = reg\n","        self.eta = eta\n","        self.l2_reg = l2_reg\n","        print(self.eta)\n","    \n","    def __call__(self, x):\n","        x_copy = x\n","\n","        x = tf.reshape(x, (x.shape[-1], -1))\n","        x = x / tf.linalg.norm(x, axis = -1, keepdims = True)\n","        cs = tf.matmul(x, tf.transpose(x))\n","        cs = cs - self.eta * (tf.eye(cs.shape[0], dtype = cs.dtype) - tf.ones_like(cs, dtype = cs.dtype))\n","\n","        cs = cs * (1 - tf.eye(cs.shape[0], dtype = cs.dtype))\n","        dist = (1 / 2.0) * tf.reduce_sum(tf.math.square(cs))\n","\n","        return (tf.cast((self.reg * dist), tf.float32) + (self.l2_reg * tf.math.reduce_sum(tf.math.square(x_copy))))\n","    \n","    def get_config(self):\n","        return {\"reg\" : self.reg,\n","                \"eta\" : self.eta,\n","                \"l2_reg\" : self.l2_reg}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jw1x7KRVYVVp","colab_type":"text"},"source":["# CUSTOM LAYERS #"]},{"cell_type":"code","metadata":{"id":"3ME3U9iTGFTQ","colab_type":"code","colab":{}},"source":["class Mish(Layer):\n","    '''\n","    Mish Activation Function.\n","    .. math::\n","        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n","    Shape:\n","        - Input: Arbitrary. Use the keyword argument `input_shape`\n","        (tuple of integers, does not include the samples axis)\n","        when using this layer as the first layer in a model.\n","        - Output: Same shape as the input.\n","    Examples:\n","        >>> X_input = Input(input_shape)\n","        >>> X = Mish()(X_input)\n","    '''\n","\n","    def __init__(self, **kwargs):\n","        super(Mish, self).__init__(**kwargs)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        return inputs * K.tanh(K.softplus(inputs))\n","\n","    def get_config(self):\n","        config = super(Mish, self).get_config()\n","        return config\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dRZGGuYwItSJ","colab":{}},"source":["class Repeat(Layer):\n","    def __init__(self, **kwargs):\n","        super(Repeat, self).__init__(**kwargs)\n","    \n","    def call(self, x):\n","        return K.repeat(x, x.shape[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fNhgXKUiItSZ","colab":{}},"source":["class Dist(Layer):\n","    def __init__(self, **kwargs):\n","        super(Dist, self).__init__(**kwargs)\n","    \n","    def call(self, x):\n","        return tf.math.reduce_mean(tf.math.abs(tf.math.subtract(x, tf.transpose(x, perm = [0, 2, 1]))), axis = -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NR2Ui57NItSl","colab":{}},"source":["class Shift(Layer): \n","    def __init__(self, **kwargs):\n","        super(Shift, self).__init__(**kwargs)\n","    \n","    def build(self, input_shape):\n","        # self.kernel = self.add_weight(\"kernel\", shape = [input_shape[1][1]])\n","        pass\n","        \n","    def call(self, x):\n","        orig_shape = tf.shape(x[0])\n","        x[0] = tf.reshape(x[0], (orig_shape[0], orig_shape[-1], -1))\n","        norms = tf.linalg.norm(x[0], axis = -1, keepdims = True)\n","\n","        shift = x[0] - x[1]\n","        shift = shift / (tf.linalg.norm(shift, axis = -1, keepdims = True) + 1e-6)\n","\n","        return tf.reshape(tf.multiply(shift, norms), orig_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02JGkHXkm_DH","colab_type":"code","colab":{}},"source":["class Grad(Layer):\n","    def __init__(self, lambd = 10.0, eps = 1e-6, **kwargs):\n","        super(Grad, self).__init__(**kwargs)\n","        self.lambd = lambd\n","        self.eps = eps\n","    \n","    def build(self, input_shape):\n","        self.kernel = self.add_weight(\"kernel\", shape = [1, input_shape[-1], 1])\n","    \n","    def call(self, x):\n","        shape = tf.shape(x)\n","        x = tf.reshape(x, (shape[0], shape[-1], -1))\n","        norms = tf.linalg.norm(x, axis = -1, keepdims = True)\n","        xn = x / (norms + 1e-6)\n","        grad = tf.matmul(xn, tf.transpose(xn, perm = (0, 2, 1)))\n","        grad = (grad * self.lambd) / (grad + tf.exp(self.lambd))\n","        grad = grad * (1 - tf.eye(tf.shape(grad)[1], dtype = grad.dtype))\n","        grad = tf.matmul(grad, xn)\n","        grad = grad / (tf.linalg.norm(grad, axis = -1, keepdims = True) + 1e-6)\n","        grad *= tf.math.square(self.kernel)\n","\n","        return grad\n","    \n","    def get_config(self):\n","        return {\"lambd\" : self.lambd, \"eps\" : self.eps}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyViGiG0kmOR","colab_type":"code","colab":{}},"source":["class RadialShift(Layer):\n","    def __init__(self, **kwargs):\n","        super(RadialShift, self).__init__(**kwargs)\n","    \n","    def call(self, x):\n","        mu = tf.reduce_mean(x, axis = -1, keepdims = True)\n","        return x - mu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmHzg9C0Ktvr","colab_type":"text"},"source":["# BLUR POOL LAYER #\n"]},{"cell_type":"code","metadata":{"id":"gArx7nnH8nBQ","colab_type":"code","colab":{}},"source":["class Downsample(Layer):\n","    def __init__(self, filt_size = 3, stride = 2, pad_off = 0, **kwargs):\n","        super(Downsample, self).__init__(**kwargs)\n","        self.filt_size = filt_size\n","        self.stride = stride\n","        self.pad_off = pad_off\n","\n","        # pad sizes: LEFT, RIGHT, TOP, BOTTOM\n","        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2)), int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]\n","        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n","\n","    def build(self, input_shape):\n","        # initialize the appropriate blur kernel\n","        if(self.filt_size == 1):\n","            a = np.array([1.,])\n","        elif(self.filt_size == 2):\n","            a = np.array([1., 1.])\n","        elif(self.filt_size == 3):\n","            a = np.array([1., 2., 1.])\n","        elif(self.filt_size == 4):    \n","            a = np.array([1., 3., 3., 1.])\n","        elif(self.filt_size == 5):    \n","            a = np.array([1., 4., 6., 4., 1.])\n","        elif(self.filt_size == 6):    \n","            a = np.array([1., 5., 10., 10., 5., 1.])\n","        elif(self.filt_size == 7):\n","            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n","        \n","        # compute the outer product to get the final filter\n","        filt = np.outer(a, a)\n","        filt = filt / np.sum(filt)\n","\n","        # kernel shape\n","        kernel_shape = (self.filt_size, self.filt_size, input_shape[3], 1)\n","\n","        # reshape the filter into the appropriate shape and create the initializer\n","        filt = np.repeat(filt, input_shape[3])\n","        filt = np.reshape(filt, kernel_shape)\n","        blur_init = keras.initializers.constant(filt)\n","\n","        # create the blur kernel\n","        self.kernel = self.add_weight(\"kernel\", shape = kernel_shape, initializer = blur_init, trainable = False)\n","\n","        # call the parent class constructor\n","        super(Downsample, self).build(input_shape)\n","    \n","    def call(self, x):\n","        if self.filt_size == 1:\n","            if self.pad_off == 0:\n","                return x[:, ::self.stride, ::self.stride, :]\n","            else:\n","                x = tf.pad(x, paddings = tf.constant([[0, 0], self.pad_sizes[2:], self.pad_sizes[:2], [0, 0]]), mode = \"REFLECT\")\n","                return x[:, ::self.stride, ::self.stride, :]\n","        else:\n","            # pad the input (reflect pad)\n","            x = tf.pad(x, paddings = tf.constant([[0, 0], self.pad_sizes[2:], self.pad_sizes[:2], [0, 0]]), mode = \"REFLECT\")\n","            return K.depthwise_conv2d(x, self.kernel, strides = (self.stride, self.stride))\n","    \n","    def get_config(self):\n","        return {\"filt_size\" : self.filt_size,\n","                \"stride\" : self.stride,\n","                \"pad_off\" : self.pad_off}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYMCWMWSbPhV","colab_type":"text"},"source":["# PREPROCESSORS #"]},{"cell_type":"code","metadata":{"id":"VssTojo5bRfr","colab_type":"code","colab":{}},"source":["class PadPreprocessor:\n","    def __init__(self, pad):\n","        # initialize the instance variables\n","        self.pad = pad\n","    \n","    def preprocess(self, img):\n","        # return the padded image\n","        return np.pad(img, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVa68bIl9iDH","colab_type":"code","colab":{}},"source":["class ReflectionPadPreprocessor:\n","    def __init__(self, pad):\n","        # initialize the instance variables\n","        self.pad = pad\n","    \n","    def preprocess(self, img):\n","        # zero pad the image\n","        img = np.pad(img, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)))\n","\n","        # reflect pad the image\n","        for i, j in zip(range(self.pad), range(self.pad)):\n","            xstart = self.pad\n","            xend = img.shape[1] - self.pad - 1\n","            ystart = self.pad\n","            yend = img.shape[0] - self.pad - 1\n","\n","            img[:, xstart - i - 1] = img[:, xstart + i + 1]\n","            img[:, xend + i + 1] = img[:, xend - i - 1]\n","            img[ystart - j - 1, :] = img[ystart + j + 1, :]         \n","            img[yend + j + 1, :] = img[yend - j - 1, :]   \n","        \n","        # return the processed image\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WF6QyWvDqPWm","colab_type":"code","colab":{}},"source":["class FlipPreprocessor:\n","    def __init__(self, prob):\n","        # initialize the instance variables \n","        self.prob = prob\n","    \n","    def preprocess(self, img):\n","        p = np.random.uniform(size = (1,))\n","\n","        # check to see if the image is to be flipped\n","        if p <= self.prob:\n","            img = cv2.flip(img, 1)\n","        \n","        # return the processed image\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BanfTB8e8e2","colab_type":"code","colab":{}},"source":["class PatchPreprocessor:\n","    def __init__(self, height, width):\n","        # initialize the instance variables - target height and width\n","        self.height = height\n","        self.width = width\n","    \n","    def preprocess(self, img):\n","        # extract a random crop from the image and return it\n","        return extract_patches_2d(img, (self.height, self.width), max_patches = 1)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9mC3MMdenCD","colab_type":"code","colab":{}},"source":["class MeanPreprocessor:\n","    def __init__(self, mean, std, normalize = True):\n","        # initialize the instance variables\n","        self.mean = mean\n","        self.std = std\n","        self.normalize = normalize\n","    \n","    def preprocess(self, img):\n","        # if the image is to be normalized, normalize it\n","        if self.normalize:\n","            img = img.astype(\"float\") / 255.0\n","\n","        # return the processed image\n","        return ((img - self.mean) / self.std)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ajD8pbeeBpn","colab_type":"code","colab":{}},"source":["class ImageToArrayPreprocessor:\n","    def __init__(self, data_format = None):\n","        # initialize the instance variables\n","        self.data_format = data_format\n","\n","    def preprocess(self, img):\n","        # apply the keras utility function that correctly rearranges the dimensions of the image\n","        return img_to_array(img, data_format = self.data_format)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfDTqx65eGz_","colab_type":"text"},"source":["# DATA GENERATOR #"]},{"cell_type":"code","metadata":{"id":"K0EUKEZVeJaN","colab_type":"code","colab":{}},"source":["class CifarGenerator:\n","    def __init__(self, x_train, y_train, batch_size, preprocessors = None, aug = None):\n","        # initialize the cifar data\n","        self.x_train = x_train\n","        self.y_train = y_train\n","\n","        # initialize the instance variables\n","        self.bs = batch_size\n","        self.preprocessors = preprocessors\n","        self.aug = aug\n","        self.num_images = self.x_train.shape[0]\n","        self.lb = LabelBinarizer()\n","        self.lb.fit(y_train)\n","    \n","    def generator(self, passes = np.inf):\n","        # initialize a variable to keep a count on the epochs\n","        epochs = 0\n","\n","        # loop through the dataset indefinitely\n","        while(epochs < passes):\n","            # initialize the indices\n","            indices = list(range(self.num_images))\n","            np.random.shuffle(indices)\n","\n","            # loop through the dataset in batches\n","            for i in range(0, self.num_images, self.bs):\n","                # extract the current indices\n","                cur_indices = sorted(indices[i : i + self.bs])\n","\n","                # grab the current batch\n","                x, y = self.x_train[cur_indices], self.y_train[cur_indices]\n","\n","                # if any preprocessors are supplied, apply them\n","                if self.preprocessors is not None:\n","                    # loop through the images\n","                    proc_x = []\n","                    for img in x:\n","                        # loop through the preprocessors\n","                        for p in self.preprocessors:\n","                            img = p.preprocess(img)\n","\n","                        proc_x.append(img)\n","                \n","                    # update the images\n","                    x = np.array(proc_x)\n","                \n","                # preprocess the labels\n","                y = self.lb.transform(y)\n","\n","                # if any augmentation is supplied, apply it\n","                if self.aug is not None:\n","                    x, y = next(self.aug.flow(x, y, batch_size = bs))\n","                \n","                # yield the current batch\n","                yield x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iBBdXoncZIp","colab_type":"code","colab":{}},"source":["class MixUpCifarGenerator:\n","    def __init__(self, x_train, y_train, batch_size, alpha = 0.4, preprocessors = None, aug = None):\n","        # initialize the cifar data\n","        self.x_train = x_train\n","        self.y_train = y_train\n","\n","        # initialize the instance variables\n","        self.bs = batch_size\n","        self.preprocessors = preprocessors\n","        self.aug = aug\n","        self.alpha = alpha\n","        self.num_images = self.x_train.shape[0]\n","        self.lb = LabelBinarizer()\n","        self.lb.fit(y_train)\n","    \n","    def generator(self, passes = np.inf):\n","        # initialize a variable to keep a count on the epochs\n","        epochs = 0\n","\n","        # loop through the dataset indefinitely\n","        while(epochs < passes):\n","            # initialize the indices\n","            indices = list(range(self.num_images))\n","            np.random.shuffle(indices)\n","\n","            # loop through the dataset in batches\n","            for i in range(0, self.num_images, self.bs):\n","                # extract the current indices\n","                cur_indices = sorted(indices[i : i + self.bs])\n","\n","                # initialize the other batch of indices\n","                if i + self.bs < self.num_images:\n","                    oth_indices = list(range(i, i + self.bs))\n","                else:\n","                    oth_indices = list(range(i, self.num_images))\n","\n","                # grab the data batches\n","                x1, y = self.x_train[cur_indices], self.y_train[cur_indices]\n","                x2 = self.x_train[oth_indices]\n","\n","                # if any preprocessors are supplied, apply them\n","                if self.preprocessors is not None:\n","                    # loop through the images\n","                    proc_x1 = []\n","                    proc_x2 = []\n","                    for img1, img2 in zip(x1, x2):\n","                        # loop through the preprocessors\n","                        for p in self.preprocessors:\n","                            img1 = p.preprocess(img1)\n","                            img2 = p.preprocess(img2)\n","\n","                        proc_x1.append(img1)\n","                        proc_x2.append(img2)\n","                \n","                    # update the images\n","                    x1 = np.array(proc_x1)\n","                    x2 = np.array(proc_x2)\n","                \n","                # randomly sample the lambda value from beta distribution.\n","                lamb = np.random.beta(self.alpha + 1, self.alpha, x1.shape[0])\n","\n","                # remove possible duplicates\n","                lamb = np.maximum(lamb, 1 - lamb)\n","\n","                # reshape the parameter to a suitable shape\n","                xlamb = lamb.reshape(-1, 1, 1, 1)\n","\n","                # perform the mixup\n","                x = (xlamb * x1) + ((1 - xlamb) * x2)\n","\n","                # preprocess the labels\n","                y = self.lb.transform(y)\n","\n","                # if any augmentation is supplied, apply it\n","                if self.aug is not None:\n","                    x, y = next(self.aug.flow(x, y, batch_size = bs))\n","                \n","                # yield the current batch\n","                yield x, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NcEQSK1pbCyD","colab_type":"text"},"source":["# MODELS #"]},{"cell_type":"code","metadata":{"id":"xY6fEFvoC2-D","colab_type":"code","colab":{}},"source":["class XResNet:\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Activation(\"relu\", name = name + \"_relu1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Activation(\"relu\", name = name + \"_relu2\")(bn2)            \n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Activation(\"relu\", name = name + \"_relu3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Activation(\"relu\", name = name + \"_relu1\")(bn1)            \n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n","\n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Activation(\"relu\", name = name + \"_relu2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red and stride != (1, 1):\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv1\")(inputs)\n","            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv2\")(x)\n","            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv3\")(x)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_max_pool\")(x)\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = XResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = XResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => RELU -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Activation(\"relu\", name = \"final_relu\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        sc = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n","        sc = Activation(\"softmax\", name = \"softmax\")(sc)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = sc, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLooEVXjGPBD","colab_type":"code","colab":{}},"source":["class MXResNet:\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)            \n","            conv2 = Conv2D(int(K * 0.25), (3, 3), padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            \n","            if stride != (1, 1):\n","                conv2 = Downsample(filt_size = 3, stride = stride[0], name = name + \"_downsample\")(conv2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                if stride != (1, 1):\n","                    shortcut = Downsample(filt_size = 3, stride = stride[0], name = name + \"_avg_blur_pool\")(act1)\n","                # shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)            \n","            conv1 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n","\n","            if stride != (1, 1):\n","                conv1 = Downsample(filt_size = 3, stride = stride[0], name = name + \"_downsample\")(conv1)\n","\n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red and stride != (1, 1):\n","                shortcut = Downsample(filt_size = 3, stride = stride[0], name = name + \"_avg_blur_pool\")(act1)\n","                # shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (1, 1), padding = \"same\", name = name + \"_stem_max_pool\")(x)\n","            x = Downsample(filt_size = 3, stride = 3, name = name + \"_stem_downsample\")(x)\n","\n","            # x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = MXResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = MXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => RELU -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        # x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"567kE02QJ5OU","colab_type":"code","colab":{}},"source":["class SPMXResNet:\n","    @staticmethod\n","    def rds_block(x, name = \"rds_block\", reg = 1e-4, lamb = 1e-3):\n","        # # shortcut = x\n","        \n","        # # x = GlobalAveragePooling2D(name = name + \"_gap\")(x)\n","        # # # x = Repeat(name = name + \"_repeat\")(x)\n","        # # # x = Dist(name = name + \"_dist\")(x)\n","        # # x = Dense(x.shape[1], use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_dist\")(x)\n","        # # x = Mish(name = name + \"_mish\")(x)\n","        # # x = Reshape((1, 1, x.shape[1]), name = name + \"_reshape\")(x)\n","        # # x = Shift(name = name + \"_shift\")([shortcut, x])\n","\n","        # # dist = Conv2D(x.shape[-1], (1, 1), kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_dist_conv\")(x)\n","        # # dist = Mish(name = name + \"_dist_mish\")(dist)\n","        # # dist = GlobalAveragePooling2D(name = name + \"_dist_gap\")(dist)\n","        # # dist = Reshape((1, 1, dist.shape[1]), name = name + \"_dist_reshape\")(dist)\n","\n","        # dist = GlobalAveragePooling2D(name = name + \"_gap\")(x)\n","        # dist = Repeat(name = name + \"_repeat\")(x)\n","        # dist = Dist(name = name + \"_dist\")(x)\n","        # dist = Reshape((1, 1, dist.shape[1]), name = name + \"_dist_reshape\")(dist)\n","\n","        # # dirn = Conv2D(x.shape[-1], (1, 1), kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_dirn_conv\")(x)\n","        # # dirn = Mish(name = name + \"_dirn_mish\")(dirn)\n","        # dirn = RadialShift(name = name + \"_dirn\")(x)\n","\n","        # x = Shift(name = name + \"_shift\")([x, dist, dirn])\n","        grad = Grad(name = name + \"_grad\")(x)\n","        x = Shift(name = name + \"_shift\")([x, grad])\n","\n","        return x\n","\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, lamb = 1e-3, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:      \n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","            # conv1 = SPMXResNet.rds_block(conv1, name = name + \"_rds1\")\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            # conv2 = SPMXResNet.rds_block(conv2, name = name + \"_rds2\")\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","            conv3 = SPMXResNet.rds_block(conv3, reg = reg, lamb = lamb, name = name + \"_rds\")\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride)(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n","            # conv1 = SPMXResNet.rds_block(conv1, name = name + \"_rds1\")\n","            \n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            conv2 = SPMXResNet.rds_block(conv2, reg = reg, lamb = lamb, name = name + \"_rds\")\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, lamb = 1e-3, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = SPMXResNet.residual_module(x, filters[i + 1], stride, chan_dim, red = True, lamb = lamb, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = SPMXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, lamb = lamb, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => MISH -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpaFKOH_B_2_","colab_type":"code","colab":{}},"source":["class SPMXResNetV2:\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = SimilarityPenalizer(reg, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3class SPMXResNetV2:\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = SimilarityPenalizer(reg, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv2\")(act2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = SimilarityPenalizer(reg, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride)(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_red\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv1\")(act1)\n","            \n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv2\")(act2)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_red\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = SPMXResNetV2.residual_module(x, filters[i + 1], stride, chan_dim, red = True, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = SPMXResNetV2.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => MISH -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(1e-4), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv2\")(act2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = SimilarityPenalizer(reg, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride)(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_red\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv1\")(act1)\n","            \n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_conv2\")(act2)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = name + \"_red\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = SimilarityPenalizer(reg, l2_reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = SPMXResNetV2.residual_module(x, filters[i + 1], stride, chan_dim, red = True, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = SPMXResNetV2.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => MISH -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(1e-4), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9dK_iuYAMtC","colab_type":"code","colab":{}},"source":["class NCMXResNet:\n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, eta, red = False, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = NegReg(reg, eta, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = name + \"_conv2\")(act2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = NegReg(reg, eta, l2_reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride)(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = name + \"_red\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = name + \"_conv1\")(act1)\n","            \n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = name + \"_conv2\")(act2)\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = name + \"_red\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, eta, stem_type = \"imagenet\", bottleneck = True, reg = 1e-1, l2_reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = NegReg(reg, eta, l2_reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = NegReg(reg, eta, l2_reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = NCMXResNet.residual_module(x, filters[i + 1], stride, chan_dim, eta = eta, red = True, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = NCMXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, eta = eta, reg = reg, l2_reg = l2_reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => MISH -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(1e-4), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQYuIOVdIZ09","colab_type":"code","colab":{}},"source":["class SEMXResNet:\n","    @staticmethod\n","    def squeeze_excite_block(tensor, ratio = 16, name = \"se_block\"):\n","        init = tensor\n","        channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n","        filters = init.shape[channel_axis]\n","        se_shape = (1, 1, filters)\n","\n","        se = GlobalAveragePooling2D(name = name + \"_gap\")(init)\n","        se = Reshape(se_shape, name = name + \"_reshape\")(se)\n","        se = Dense(filters // ratio, kernel_initializer = 'he_normal', use_bias = False, name = name + \"_squeeze\")(se)\n","        se = Activation(\"relu\", name = name + \"_squeeze_relu\")(se)\n","        se = Dense(filters, kernel_initializer = 'he_normal', use_bias = False, name = name + \"_excite\")(se)\n","        se = Activation(\"sigmoid\", name = name + \"_excite_sigmoid\")(se)\n","\n","        if K.image_data_format() == 'channels_first':\n","            se = Permute((3, 1, 2))(se)\n","\n","        x = multiply([init, se], name = name + \"_scale\")\n","        return x\n","    \n","    @staticmethod\n","    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n","        # shortcut branch\n","        shortcut = data\n","\n","        if bottleneck:\n","            # first bottleneck block - 1x1\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n","\n","            # conv block - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            \n","            # second bottleneck block - 1x1\n","            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n","            act3 = Mish(name = name + \"_mish3\")(bn3)\n","            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n","\n","            # se module\n","            conv3 = SEMXResNet.squeeze_excite_block(conv3, name = name + \"_se_block\")\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride)(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","            \n","            # add the shortcut and final conv\n","            x = add([conv3, shortcut], name = name + \"_add\")\n","        \n","        else:\n","            # conv block 1 - 3x3\n","            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n","            act1 = Mish(name = name + \"_mish1\")(bn1)\n","            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n","            \n","            # conv block 2 - 3x3\n","            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n","            act2 = Mish(name = name + \"_mish2\")(bn2)\n","            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n","                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n","            \n","            # se module\n","            conv2 = SEMXResNet.squeeze_excite_block(conv2, name = name + \"_se_block\")\n","\n","            # if dimensions are to be reduced, apply a conv layer to the shortcut\n","            if red:\n","                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n","                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n","\n","            # add the shortcut and final conv\n","            x = add([conv2, shortcut], name = name + \"_add\")      \n","\n","        # return the addition as the output of the residual block\n","        return x\n","\n","    @staticmethod\n","    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n","        # set the input shape\n","        if K.image_data_format() == \"channels_last\":\n","            input_shape = (height, width, depth)\n","            chan_dim = -1\n","        else:\n","            input_shape = (depth, height, width)\n","            chan_dim = 1\n","\n","        # initialize a counter to keep count of the total number of layers in the model\n","        n_layers = 0\n","        \n","        # input block\n","        inputs = Input(shape = input_shape)\n","\n","        # stem\n","        if stem_type is \"imagenet\":\n","            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n","                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_pool\")(x)\n","\n","        elif stem_type is \"cifar\":\n","            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n","                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        # modify the stages to suit bottleck\n","        if bottleneck:\n","            stages = [int(np.floor(st / 3)) for st in stages]\n","        else:\n","            stages = [int(np.floor(st / 2)) for st in stages]\n","\n","        # loop through the stages\n","        for i in range(0, len(stages)):\n","            # set the stride value\n","            stride = (1, 1) if i == 0 else (2, 2)\n","\n","            name = f\"stage{i + 1}_res_block1\"\n","            x = SEMXResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # loop through the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a residual module\n","                name = f\"stage{i + 1}_res_block{j + 2}\"\n","                x = SEMXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n","\n","            # increment the number of layers\n","            if bottleneck:\n","                n_layers += (3 * stages[i])\n","            else:\n","                n_layers += (2 * stages[i])\n","        \n","        # BN => MISH -> POOL\n","        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n","        x = Mish(name = \"final_mish\")(x)\n","        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n","        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n","        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n","\n","        # softmax classifier\n","        x = Flatten(name = \"flatten\")(x)\n","        x = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n","        x = Activation(\"softmax\", name = \"softmax\")(x)\n","\n","        # increment the number of layers\n","        n_layers += 1\n","\n","        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n","\n","        # return the constructed network architecture\n","        return Model(inputs = inputs, outputs = x, name = f\"{__class__.__name__}{n_layers}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aIcKqUyEHKW","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1593929389134,"user_tz":-330,"elapsed":2026,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"3d92b0aa-2ee0-4a46-b6a1-5cabc8160319"},"source":["MODELS = {\n","    \"xresnet20\" : XResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n","    # \"mxresnet20\" : MXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n","    # \"se-mxresnet20\" : SEMXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, reg = 1e-4)\n","    # \"se-mxresnet32\" : SEMXResNet.build(32, 32, 3, 10, [10, 10, 10], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, reg = 1e-4)\n","    # \"sp-mxresnet20\" : SPMXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, reg = 1e-4)\n","    # \"sp-mxresnet20v2\" : SPMXResNetV2.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, reg = 1e-2, l2_reg = 1e-4)\n","    # \"nc-mxresnet20\" : NCMXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, eta = 1.0, reg = 1e-3, l2_reg = 1e-4)\n","    # \"xresnet32\" : XResNet.build(32, 32, 3, 10, [10, 10, 10], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n","    # \"resnet44\" : ResNet.build(32, 32, 3, 10, [14, 14, 14], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n","    # \"resnet56\" : ResNet.build(32, 32, 3, 10, [18, 18, 18], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False)\n","}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] XResNet20 built successfully!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UW-Kq4s0sCj8","colab_type":"text"},"source":["# CALLBACKS #"]},{"cell_type":"code","metadata":{"id":"d-58a06NsE55","colab_type":"code","colab":{}},"source":["class TrainingMonitor(BaseLogger):\n","    def __init__(self, fig_path, json_path = None, start_at = 0):\n","        # store the output path for the figure, the path to the JSON serialized file, and the starting epoch\n","        super(TrainingMonitor, self).__init__()\n","        self.fig_path = fig_path\n","        self.json_path = json_path\n","        self.start_at = start_at\n","\n","    def on_train_begin(self, logs = {}):\n","        # initialize the history dictionary\n","        self.H = {}\n","\n","        # if the JSON history path exists, load the training history\n","        if self.json_path is not None:\n","            if os.path.exists(self.json_path):\n","                self.H = json.loads(open(self.json_path).read())\n","\n","                # check to see if a starting epoch was supplied\n","                if self.start_at > 0:\n","                    # loop over the entries in the history log and trim any entries that are past the starting epoch\n","                    for key in self.H.keys():\n","                        self.H[key] = self.H[key][:self.start_at]\n","\n","    def on_epoch_end(self, epoch, logs = {}):\n","        # loop over the logs and update the loss, accuracy etc, for the entire training process\n","        for (k, v) in logs.items():\n","            l = self.H.get(k, [])\n","            l.append(v)\n","            self.H[k] = l\n","\n","        # check to see if the training history should be serialized to file\n","        if self.json_path is not None:\n","            f = open(self.json_path, \"w\")\n","            f.write(json.dumps(self.H))\n","            f.close()\n","\n","        # ensure atleast two epochs have passed before plotting\n","        if len(self.H[\"loss\"]) > 1:\n","            # plot the training loss and accuracy\n","            N = np.arange(0, len(self.H[\"loss\"]))\n","            plt.figure()\n","            plt.style.use(\"ggplot\")\n","            plt.plot(N, self.H[\"loss\"], label = \"train_loss\")\n","            plt.plot(N, self.H[\"val_loss\"], label = \"val_loss\")\n","            plt.plot(N, self.H[\"accuracy\"], label = \"acc\")\n","            plt.plot(N, self.H[\"val_accuracy\"], label = \"val_acc\")\n","            plt.title(\"Training Loss [Epoch {}]\".format(len(self.H[\"loss\"])))\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Loss\")\n","            plt.legend()\n","\n","            # save the figure\n","            plt.savefig(self.fig_path)\n","            plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnAWgSTX0Kx-","colab_type":"code","colab":{}},"source":["class LearningRateFinder:\n","    def __init__(self, model, stop_factor = 4, beta = 0.98):\n","        # initialize the instance variables\n","        self.model = model\n","        self.stop_factor = stop_factor\n","        self.beta = beta\n","\n","        # initialize the list of learning rates and losses\n","        self.lrs = []\n","        self.losses = []\n","\n","        # initialize the rest of the required variables\n","        self.lr_mult = 1\n","        self.avg_loss = 0\n","        self.best_loss = 1e9\n","        self.batch_num = 0\n","        self.weights_file = None\n","    \n","    def reset(self):\n","        # reset the instance variables\n","        self.lrs = []\n","        self.losses = []\n","        self.lr_mult = 1\n","        self.avg_loss = 0\n","        self.best_loss = 1e9\n","        self.batch_num = 0\n","        self.weights_file = None\n","    \n","    def is_data_iter(self, data):\n","        # define the set of classes types to be checked for\n","        iter_classes = [\"NumpyArrayIterator\", \"DirectoryIterator\", \"DataFrameIterator\",\n","                        \"Iterator\", \"Sequence\", \"generator\"]\n","\n","        # return whether the data is an interator\n","        return data.__class__.__name__ in iter_classes\n","    \n","    def on_batch_end(self, batch, logs):\n","        # add the current learning rate to the list of lrs\n","        lr = K.get_value(self.model.optimizer.lr)\n","        self.lrs.append(lr)\n","\n","        # grab the loss at the end of the current batch, increment the total number of batches processed,\n","        # compute the average, average loss, smooth it and update the losses list with the smoothed value\n","        l = logs[\"loss\"]\n","        self.batch_num += 1\n","        self.avg_loss = (self.beta * self.avg_loss) + ((1 - self.beta) * l)\n","        smooth = self.avg_loss / (1 - (self.beta ** self.batch_num))\n","        self.losses.append(smooth)\n","\n","        # compute the stopping loss value\n","        stop_loss = self.stop_factor * self.best_loss\n","\n","        # check if the loss has grown to large\n","        if (self.batch_num > 1) and (smooth > stop_loss):            \n","            # stop training and return from the method\n","            self.model.stop_training = True\n","            return\n","        \n","        # check if the best loss is to be updated\n","        if self.batch_num == 1 or smooth < self.best_loss:\n","            self.best_loss = smooth\n","        \n","        # increase the learning rate\n","        lr *= self.lr_mult\n","        K.set_value(self.model.optimizer.lr, lr)\n","    \n","    def find(self, train_data, start_lr, stop_lr, epochs = None, steps_per_epoch = None, bs = 32,\n","             sample_size = 2048, verbose = 1):\n","        # reset the instance variables\n","        self.reset()\n","\n","        # determine if the data supplied is a generator\n","        use_gen = self.is_data_iter(train_data)\n","\n","        # if a generator is used, and steps per epoch is not supplied, raise an error\n","        if use_gen and steps_per_epoch is None:\n","            msg = \"[ERROR] using generator without supplying steps per epoch\"\n","            raise Exception(msg)\n","        # if the generator is not used, entire dataset is in memory\n","        elif not use_gen:\n","            # calculate the steps per epoch\n","            num_samples = train_data[0].shape[0]\n","            steps_per_epoch = np.ceil(num_samples / float(bs))\n","        \n","        # if epochs is not specified, calculate the number of epochs\n","        if epochs is None:\n","            epochs = int(np.ceil(sample_size / float(steps_per_epoch)))\n","        \n","        # compute the total number of batch updates\n","        num_batch_updates =  epochs * steps_per_epoch\n","\n","        # compute the lr multiplier\n","        self.lr_mult = (stop_lr / start_lr) ** (1.0 / num_batch_updates)\n","\n","        # create a temporary file to store the model weights\n","        self.weights_file = tempfile.mkstemp()[1]\n","        self.model.save_weights(self.weights_file)\n","\n","        # create a copy of the original lr to be reset later\n","        orig_lr = K.get_value(self.model.optimizer.lr)\n","        K.set_value(self.model.optimizer.lr, start_lr)\n","\n","        # construct a callback, to increase the learning rate as training progresses\n","        callback = LambdaCallback(on_batch_end = lambda batch, logs: self.on_batch_end(batch, logs))\n","\n","        # start training according to the data specified\n","        if use_gen:\n","            self.model.fit(\n","                x = train_data,\n","                steps_per_epoch = steps_per_epoch,\n","                epochs = epochs,\n","                verbose = verbose,\n","                callbacks = [callback]\n","            )\n","        else:\n","            self.model.fit(\n","                x = train_data[0], y = train_data[1],\n","                batch_size = bs,\n","                epochs = epochs,\n","                callbacks = [callback],\n","                verbose = verbose\n","            )\n","        \n","        # restore the original model weights and learning rate\n","        self.model.load_weights(self.weights_file)\n","        K.set_value(self.model.optimizer.lr, orig_lr)\n","    \n","    def plot_loss(self, skip_begin = 10, skip_end = 1, title = \"\"):\n","        # grab the learning rate and loss values to plot\n","        lrs = self.lrs[skip_begin : -skip_end]\n","        losses = self.losses[skip_begin : -skip_end]\n","\n","        # plot the loss vs learning rate\n","        plt.style.use(\"ggplot\")\n","        plt.plot(lrs, losses)\n","        plt.xscale(\"log\")\n","        plt.xlabel(\"Learning Rate [log scale]\")\n","        plt.ylabel(\"Loss\")\n","\n","        # if title is not empty, add it to the plot\n","        if title != \"\":\n","            plt.title(title)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyEkFtoGr_7f","colab_type":"text"},"source":["# TRAINING #"]},{"cell_type":"code","metadata":{"id":"8NPeYUDos98I","colab_type":"code","colab":{}},"source":["# initialize the dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# split the dataset into the train and validation splits\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n","                                                  test_size = 0.1, random_state = 42,\n","                                                  stratify = y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCsQhhpWtrEY","colab_type":"code","colab":{}},"source":["# define training constants\n","epochs = 180\n","bs = 128\n","steps_per_epoch = np.ceil(x_train.shape[0] / bs)\n","validation_steps = np.ceil(x_val.shape[0] / bs)\n","test_steps = np.ceil(x_test.shape[0] / bs)\n","model_name = \"xresnet20\"\n","init_lr = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_w3A34Qs31r","colab_type":"code","colab":{}},"source":["# initialize the preprocessors\n","# rpp = ReflectionPadPreprocessor(4)\n","pp = PadPreprocessor(4)\n","fp = FlipPreprocessor(0.5)\n","patchp = PatchPreprocessor(32, 32)\n","mp = MeanPreprocessor([0.4914, 0.4822, 0.4465], [0.247, 0.2435, 0.2616])\n","iap = ImageToArrayPreprocessor()\n","\n","# initialize the data generators\n","train_datagen = CifarGenerator(x_train, y_train, bs, preprocessors = [pp, fp, patchp, mp, iap]).generator()\n","val_datagen = CifarGenerator(x_val, y_val, bs, preprocessors = [mp, iap]).generator()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrMNWBBI0krK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":115},"executionInfo":{"status":"ok","timestamp":1593929468612,"user_tz":-330,"elapsed":65760,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"70f9f70e-f50d-4656-e793-69807753e6f6"},"source":["lr_model = MODELS[model_name]\n","opt = SGD(lr = init_lr, momentum = 0.9)\n","lr_model.compile(loss = \"categorical_crossentropy\", optimizer = opt)\n","lrf = LearningRateFinder(lr_model)\n","lrf.find(train_datagen, 1e-10, 1e+2, steps_per_epoch = steps_per_epoch, sample_size = 1024)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","352/352 [==============================] - 21s 58ms/step - loss: 3.5726\n","Epoch 2/3\n","352/352 [==============================] - 21s 58ms/step - loss: 2.9216\n","Epoch 3/3\n","352/352 [==============================] - 20s 57ms/step - loss: 174108827342608531456.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"73uS1fgmmKEV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1593929527380,"user_tz":-330,"elapsed":1401,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"7c3d22c2-19df-4ea1-badd-56dd90fb6d6d"},"source":["lrf.plot_loss(skip_end = 40)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7Q3ZoL9mfPyB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1593927811720,"user_tz":-330,"elapsed":1619,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"4222387c-5dfc-438e-800a-cc95e0115826"},"source":["lrf.plot_loss(skip_end = 250)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAENCAYAAAD+CUlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZd7G8e9zMum9EiCwEnqVuhSVpQQpYsMVlLIUUSkWfF1XxHWxiwIWJIguCIqCuKjrWtYSXEQFhEgzoS2dGDCkkZBAypzn/WPcSKRMAsmcmczvc125nMycM3M/GZx7Tldaa40QQghxAYbVAYQQQrg/KQshhBBOSVkIIYRwSspCCCGEU1IWQgghnJKyEEII4ZSUhRBCCKdsVge4VJmZmVZHqJKYmBiys7OtjlHrvGGc3jBGkHHWNWeOs0GDBtWeX5YshBBCOCVlIYQQwikpCyGEEE5JWQghhHBKykIIIYRTUhZCCCGc8sqy0KUlmN9/jbbb0aZpdZxap3OPo/fuQGf/jDbtVscRQnggjz/O4mLo779GvzkfvWguKIXq1Q81chLKz9/qaDVGmyb8mIr5xQewJ/3XB/wD4XdNUdGx4OsHhgHql+8MJwvQhSeg8AQU5ENZKQQEQmAwRMWgoutBTBxExaKi4xy3wyJRhld+5xDCq3hlWagrklCBQegjB6CoEL32c/SRgxg3j0e16mBpNl1y2vFBfbLAka3oJBSfhKJCKCqCU0VQhetV6T1pkP2z40N+2FhUwmXovGzIOIA+uBe9J91RBqbp+AEICYXQcIirj2raGnx9oeQ0uqgQcrPRh/Y5cgEVCWw2iIyB6DhUXH2KmrZEh0ZAVCxExqCCgmvnDyWEcCnl6VfKq4kjuPWWDZjLX4X8HGjZHnXV1aiW7SA8CqWU8/ntdse8OVlQdBJdVur4IC4rhdJSKD1NoI8Pp07kQ2mJ4wO4tMRx+5ffKS5yfKMvLTn/C/kHQmCQY2ngwonA1x919fWoXkkoW819J9CnT0HOccjNQudkOW7n/HI7KxNOFv4mcwCERTh+QiNQYeG/LtEYPo7/+vj8etvwAZ///dfn18d8bODjg7L5OkrM5gtaY376Dzh+FFAQ3xDjupGopq1qbLzn4o1H/NZl3jjOizmCW8riF7qsFP31Z+jP3ocTuY47Q8MhOg7CIlDhkb984IU7PhBzfkb/8kFJXvav387Px+YLfn7g53/OHxUUAmHhjucPDUeFhDu+6QeHQFAIBAU7PijdXJSfjZwd29G52Y6/S34eFOSjC/MdS0wF+WAvB7sJpt3xd7OXX9qLduiGCgpB79oGJ/JQ/YaibvwTyr92Vit644dLXeaN47yYsvDK1VDnonz9UEnXoftdAwf3og/sgSMH0Pk5kJeNPrQXCk6ANkEpiIh2rHpp1hp+WZevomMhJAx8/R3F4Ov7y21/YuvV84p/kEZYBKpZG5wvj1WmzTPLw+64bbefcbvccbusDMrP+AkIQiW2dDzH6WL0+2+iV3+ETt+MMeH/UE2a1/wghfBCUha/oQwfSGxZ8QF0Jm3aoeik4wPK1/2/5XsSZRhVWL3m5DkCglAjJ6E79sBc8hLmM39G/WEQ6vpRqJCwGkoqhHeS3ViqQRk+qNBwKQo3p9p0xHhsPqrfUPTXn2POuAPzo3fQp4qtjiaEx5KyEHWSCgrGuOV2jJnzoFUH9L+WY864HfPzDxw7FwghqkXKQtRpqmFjfKbMwJgxFxo3Q69agjnjTsz/fIouL7M6nhAeQ8pCeAXVpDk+9z2G8cDTEBuPXr4Q87F70bvTrI4mhEeQshBeRbVoh/GXZzDuegTKSjHnzMB8/UXHketCiPOSvaGE11FKweXdMFp1QH+yEv3FB+jtm1A3jXUc3S+nLxHiLPJ/hfBayt8fY9ifMB55CRo0Qr85H3P2DPRPh6yOJoTbkbIQXk81bIzxwDOocffAsSOYT0zDfO8NdInsNSXE/0hZCIFj1ZRxRRLG46+gevRBf/Ye5syp6O2brI4mhFtw6TYL0zSZPn06UVFRTJ8+vdJjZWVlzJ8/n/379xMaGsq0adOIi4tzZTwhUKFhqHH3onv1x3zrFcyXn4DOPTFG3I6KirE6nhCWcemSxaeffkrDhg3P+dhXX31FcHAwL7/8Mtdccw1vv/22K6MJUYlq0Q7jby+ibhwDP/6A+bepmCkfOs4wLIQXcllZ5OTksHnzZvr373/Ox1NTU+nTpw8APXr0IC0tDQ8/Ia7wcMrmizHkZozH5kPzNuiVizGfvp+yPTusjiaEy7lsNdTSpUsZPXo0p06dOufjubm5REdHA+Dj40NQUBCFhYWEhVU+AVxKSgopKSkAzJo1i5gYz1g1YLPZPCbrpaiT44yJQT8+j5L1/6Fw8YvkTr+dwIE3EvKnyRiBdffiTnXyvTwHGWcV56/BLOf1ww8/EB4eTmJiIunp6c5nuICkpCSSkpIqfveU03574znz65wWHeDR+QR+/h6nPn2PU1s3YkyZgaqfYHWyWlGn38szeOM4L+Z6Fi5ZDbV7925SU1OZOnUqL774ImlpacybN6/SNFFRUeTk5ABgt9spLi4mNDTUFfGEqDIVGETYxPsw/u9xKCrEfPp+9JYNVscSota5pCxGjhzJwoULSU5OZtq0abRr14577rmn0jRdunRhzZo1AGzYsIG2bdtW6ZKmQlhBteqA8dfnoX4jzAVPY6Z8aHUkIWqVpcdZrFy5ktTUVAD69evHyZMnufvuu/n4448ZNWqUldGEcEpFxTpOTNi5p2Pj96qlslOGqLPkGtwu4o3rReuq345Rm3b0itfQa/6N6tkX9ae7UTbPP+2aN7yX4J3jlGtwC2EBZfjAyEkQHon+cDn6ZCHG5OkoXz+rowlRY+R0H0LUAKUUxtBbUGOmQNoPmMlPoctKrY4lRI2RshCiBhm9B6H+dBekb8Fc8LQUhqgzpCyEqGHGlQMchZG2GfOVWXL5VlEnSFkIUQuMq65GjZ4CP6ail8xDm6bVkYS4JLKBW4haYvxhEObJAvQ/34LQMBgxUY4dEh5LykKIWqSG3AyFJ9CrP4KwCMfvQnggKQshapFSCobfBoUF6A+WYYaGY1x1tdWxhKg2KQshapkyDBh/D7q4EL1sATo4FNW5p9WxhKgW2cAthAsomy/GpOlwWTPMv89B7/7R6khCVIuUhRAuovwDMO75G8TGOw7aO7zf6khCVJmUhRAupELCMKY9CoFBmC89is46anUkIapEykIIF1NRsRjTHgfTjvniTPSJPKsjCeGUlIUQFlD1EzDu/hucyMN88VF0cZHVkYS4ICkLISyiEltiTH4Ijh6WEw8KtydlIYSFVLvOqPHTYE8a5mtz0Ha71ZGEOCcpCyEsZnT/A+qW22HrBvTbr8jV9oRbkoPyhHADRv9rMQtOoD99F0LDUTeOsTqSEJVIWQjhJtQNo6AwH/3pPxynBUm6zupIQlSQshDCTSilYPRk9MkC9MpFmCFhGD36WB1LCEC2WQjhVpThg3H7n6FFO/TSl9BpP1gdSQhAykIIt6N8/TCmPgwNGjuutLdvl9WRhJCyEMIdqaBgx2lBwiMx5z2O/umQ1ZGEl5OyEMJNqbBIjPseBz8/R2HIpVmFhaQshHBjKjYeNegmyD0OJ09YHUd4MSkLIdycio513MjNtjaI8GpSFkK4u8j/lcVxa3MIryZlIYS7i4oBQMuShbCQlIUQ7i4kDHz9IE/KQlhHykIIN6eUgsgY2WYhLCVlIYQniIpByzYLYSEpCyE8gIqKlSULYSmXnEiwtLSUmTNnUl5ejt1up0ePHgwfPrzSNGvWrGHZsmVERUUBMGjQIPr37++KeEK4v6gYOJGHtttRPj5WpxFeyCVl4evry8yZMwkICKC8vJy//e1vdOzYkRYtWlSarlevXtx2222uiCSEZ4mMAW1Cfi7877gLIVzIJauhlFIEBAQAYLfbsdvtjo12QogqUVFyrIWwlsuuZ2GaJg8++CDHjh1j4MCBNG/e/Kxpvv/+e3bu3En9+vUZO3YsMTExroonhHv7ZWlC52ShmrexOIzwRkq7+IK/RUVFzJkzh/Hjx9O4ceOK+wsLCwkICMDX15cvv/ySdevWMXPmzLPmT0lJISUlBYBZs2ZRWlrqsuyXwmazUV5ebnWMWucN47RijLq0hKxb+hE84jZCRkxwyWt6w3sJ3jlOPz+/6s9f04GcCQ4Opm3btmzdurVSWYSGhlbc7t+/P2+99dY5509KSiIpKani9+xsz9hDJCYmxmOyXgpvGKdlY4yIpvjQPk676LW94b0E7xxngwYNqj2/S7ZZFBQUUFRUBDj2jNq+fTsNGzasNE1eXl7F7dTUVBISElwRTQjPERuPPn7M6hTCS7lkySIvL4/k5GRM00RrTc+ePenSpQsrV66kadOmdO3alX//+9+kpqbi4+NDSEgIU6ZMcUU0ITyGio2Xy6wKy7ikLH73u9/x3HPPnXX/iBEjKm6PHDmSkSNHuiKOEJ4pNt5xrEXJaZR/gNVphJeRI7iF8BRx9R3/PX7U2hzCK0lZCOEhVGy840aWbLcQridlIYSniHUsWchGbmEFKQshPIQKDoGgEFkNJSwhZSGEJ5HdZ4VFpCyE8CAqrj5IWQgLSFkI4Uni6kNOFrqszOokwstIWQjhSeITwDQhS7ZbCNeSshDCg6j6jRw3jh2xNojwOlIWQniSeMc51fRRKQvhWlIWQngQ5R8A0XFwNMPqKMLLSFkI4WnqJ8iShXA5KQshPIyKbwQ//4Q2TaujCC8iZSGEp6mfAKWlkJNldRLhRaQshPAwFXtEZcqqKOE6UhZCeJqEywDQGQeszSG8ipSFEB5GBQY5zhF1ZL/VUYQXkbIQwhM1SoQjsmQhXEfKQggPpBo1gayj6NPFVkcRXkLKQggPpBolOm4cOWhpDuE9pCyE8ESNHWUh2y2Eq0hZCOGJIqIgJEy2WwiXkbIQwgMppaBxIvrgXqujCC8hZSGEh1KJLeGnQ+jTp6yOIryAlIUQHkoltgJtwsH/Wh1FeIEql0VaWhpZWY5z0eTl5TF//nwWLFhAfn5+rYUTQlxAYgsA9P7dFgcR3qDKZbF48WIMwzH5m2++id1uRynFq6++WmvhhBDnp4JDIb6hlIVwCVtVJ8zNzSUmJga73c62bdtYsGABNpuNO++8szbzCSEuQDVpif4xFa21Y6O3ELWkyksWgYGB5Ofns2PHDhISEggICACgvLy81sIJIZxo2gpOFsDxo1YnEXVclZcsBg0axEMPPUR5eTnjxo0DYNeuXTRs2LC2sgkhnFDN2qABvScdFdfA6jiiDqtyWdxwww38/ve/xzAM4uPjAYiKimLSpEm1Fk4I4USDRhAaDrt/hCsHWJ1G1GFVLguABg1+/eaSlpaGYRi0adOmxkMJIapGKYVq1QG9a7tstxC1qsplMXPmTG699VZatWrFP//5Tz755BMMw2DgwIEMGzbsgvOWlpYyc+ZMysvLsdvt9OjRg+HDh1eapqysjPnz57N//35CQ0OZNm0acXFxFzcqIbxJq/aw6Rv4ORPiZbWwqB1V3sB95MgRWrRw7Ne9evVqZs6cyVNPPcWXX37pdF5fX19mzpzJ7Nmzee6559i6dSt79uypNM1XX31FcHAwL7/8Mtdccw1vv/12NYcihHdSLTsAoHdttziJqMuqXBZaawCOHTsGQEJCAjExMRQVFTmdVylVsfeU3W6vOEbjTKmpqfTp0weAHj16kJaWVvGaQogLiKsPkTHoXdusTiLqsCqvhmrZsiWvv/46eXl5dOvWDXAUR2hoaJXmN02TBx98kGPHjjFw4ECaN29e6fHc3Fyio6MB8PHxISgoiMLCQsLCwqoaUQivpJRCtemI3rweXV6OslVrU6QQVVLlf1VTp07lo48+IiwsjOuuuw6AzMxMhgwZUqX5DcNg9uzZFBUVMWfOHA4fPkzjxo2rHTglJYWUlBQAZs2aRUxMTLWfwwo2m81jsl4KbxinO47x9JX9OfFdCuHZR/Fr16lGntMdx1kbZJxVnL+qE4aGhjJy5MhK93Xu3LnaLxgcHEzbtm3ZunVrpbKIiooiJyeH6Oho7HY7xcXF51xqSUpKIikpqeL37OzsamewQkxMjMdkvRTeME53HKNOaAI+NvK/TcGIb1Qjz+mO46wN3jjOM/dsraoql0V5eTnvv/8+a9euJS8vj8jISHr37s2wYcOwOVnsLSgowMfHh+DgYEpLS9m+fTvXX399pWm6dOnCmjVraNGiBRs2bKBt27ayG6AQVaQCgqBFW/T2VPjjeKvjiDqoymXx1ltvsW/fPm6//XZiY2M5fvw47733HsXFxRVHdJ9PXl4eycnJmKaJ1pqePXvSpUsXVq5cSdOmTenatSv9+vVj/vz53H333YSEhDBt2rRLHZsQXkV16IpeuRh9/BgqNt7qOKKOUbqKuxxNmjSJ2bNnV1o1VFBQwAMPPGDpmWczMzMte+3q8MZF3brKXceoszIxH56EGn4bxoDrnc/ghLuOs6Z54zgvZjVUtXedFUK4JxXXwHGp1U3fWB1F1EFVLouePXvy7LPPsnXrVjIyMti6dSuzZ8+mZ8+etZlPCFENqutVcGAP+vgxq6OIOqbK2yxGjx7Ne++9x+LFi8nLyyMqKopevXrJKcqFcCOq25Xo999Ap36HGnyT1XFEHVLlsrDZbIwYMYIRI0ZU3FdaWsqYMWMYPXp0rYQTQlSPiqkHTVqgN60FKQtRg6q8GupcZNdWIdyP6v4HOHIAnXHA6iiiDrmkshBCuB/Vow/YfNFrv7A6iqhDnK6GSktLO+9jsr1CCPejgkNRXXqhN6xB3zQO5e9vdSRRBzgti1deeeWCj3vDOVWE8DTqqoHo779G//Atqld/q+OIOsBpWSQnJ7sihxCiJrVoC/EJ6K8+QffsJ9sXxSWTbRZC1EFKKdSA6+HQXpCLIokaIGUhRB2levaFsAjMz963OoqoA6QshKijlK8fqv+1sGML+tA+q+MIDydlIUQdpvoMgaAQzA/etDqK8HBSFkLUYSooGHXNzZC+Bb1TrtEtLp6UhRB1nOp7DUTFYq5aijbtVscRHkrKQog6Tvn6oYb9CQ7vQ3/9mdVxhIeSshDCC6jf94Y2HdHvv4nOz7E6jvBAUhZCeAGlFMaoyWC3Y769UC5mJqpNykIIL6Hi6qNuGAVbv0d/87nVcYSHkbIQwouopOuhTSf0ykXozMNWxxEeRMpCCC+iDANjwjTwD8RMfhpddNLqSMJDSFkI4WVUeCTG5IcgNwvz1WfRcqkBUQVSFkJ4IdW8DWrMVNi5Db1cNngL56p8DW4hRN1i9OqPmXUU/cm74B8IwyfIqczFeUlZCOHF1PWj4PQpdMqH4O8P14+SwhDnJGUhhBdTSsHw26C0xLGEcfoUDL8NZcgaalGZlIUQXk4ZBoyeAv6BjiWMgnwYP83qWMLNSFkIIRyFMXwCRESiVy1F5x7HPuM5q2MJNyLLmkII4JdTggwchrrjL3DkALl/noDet8vqWMJNSFkIISoxul2J8dBslJ8f5uwZmF98gDZNq2MJi0lZCCHOohIuI2rO69C+K/ofSzBfnInOk7PVejMpCyHEORkhYRhTHnIcvLdvF+Zj96BTv5UD+LyUlIUQ4ryUUhi9B2I88iLE1MN89TnMBU/LUoYXcsneUNnZ2SQnJ5Ofn49SiqSkJIYMGVJpmvT0dJ577jni4uIA6N69O3/84x9dEU8I4YSKb4jx0Gx0yofoD5djzpyKGjYW1XugHJPhJVxSFj4+PowZM4bExEROnTrF9OnT6dChAwkJCZWma926NdOnT3dFJCFENSkfH9TAYehOPTHfWoB++xX0dykYI+9ENWlhdTxRy1zylSAyMpLExEQAAgMDadiwIbm5ua54aSFEDVNx9THuexx1232Ql4P59J8xl76ELsizOpqoRUq7eGtVVlYWM2fOZO7cuQQFBVXcn56ezty5c4mOjiYyMpIxY8bQqFGjs+ZPSUkhJSUFgFmzZlFaWuqy7JfCZrNR7gWngvaGcXrDGKFq4zRPFVH07lKKP16J8vMn+JaJBA2+CWXznON9vfH99PPzq/b8Li2L06dPM3PmTIYNG0b37t0rPVZcXIxhGAQEBLB582aWLl3KvHnznD5nZmZmbcWtUTExMWRnZ1sdo9Z5wzi9YYxQvXHqYxmYKxdB2mZo0BjjlttRrS+v5YQ1wxvfzwYNGlR7fpdtmSovL2fu3LlcddVVZxUFQFBQEAEBAQB07twZu91OQUGBq+IJIS6Bik/AuGcmxtSHobQE8/lHsL8yC338mNXRRA1xybKi1pqFCxfSsGFDhg4des5p8vPzCQ8PRynF3r17MU2T0NBQV8QTQtQApRR07I7RthP68w/Q/16FuX0jqt+1qGtuRgWFWB1RXAKXlMXu3btZu3YtjRs35oEHHgDg1ltvrVgkuvrqq9mwYQNffPEFPj4++Pn5MW3aNDmvvhAeSPn6oYaOQF+ZhP7n2+gv/4lel4K69lZU70EetT1D/MrlG7hrmmyzcC/eME5vGCPU3Dj14f2Y/3gddm2H+IYYf5wAHbq6zZdBT3w/zZWLUe27oNp0rPI8HrPNQgjhnVTjRIz/ewLjrr+CBnP+E5gv/A195IDV0TyS3rHVcXDk4X0ufV1ZHhRC1DqlFFz+e4y2ndFff4b+aAXmE9NQvfqjbhiNioiyOqJH0KbpWEqLqYfqf51LX1vKQgjhMspmQ/Ufiu7RB/3pu+jVH6NTv0UNugl19Q0oP3+rI7o1/cN3kHEQNfF+lK+vS19bVkMJIVxOBYdg3DwB4/H50LYT+sO3MR+ZjN62yepobkubdvRH70D9RqhuV7r89aUshBCWUXEN8Jn8EMafnwb/QMzFc9FecDT1xdCp38HRI6hrb0EZPi5/fSkLIYTlVMt2GDeMglPFsH+31XHcjtYa/fn7EJ+A6nKFJRmkLIQQ7qFVBzAM9I4tVidxP/t2wuH9qP7XWnZKeCkLIYRbUEEh0KQFOl3K4rf06o8hMBjVs69lGaQshBBuQ7XtDIf2ogvlvHD/o3Oz0ZvXoa4agPIPsCyHlIUQwm2oNh1Ba/SubVZHcRt63WowTVSfIc4nrkVSFkII99GkOQSFQPpmq5O4DZ36LTRrg4qNtzSHlIUQwm0owwfV+nJ0+lY8/LR1NUJnHoafDllyXMVvSVkIIdxL206QnwOZR6xOYjm96VtQhmW7y55JykII4VZU204AaC9fFaW1Rqd+Ay3aosIjrY4jZSGEcC8qKhbqN5LjLTIOwrGfUF2tXwUFUhZCCDek2nSEPeno06esjmIZnfotGAaqSy+rowBSFkIIN6Q694KyUvS2jVZHsYTWGr3pG2jVARUabnUcQMpCCOGOmrWGiGjHB6Y3OrwPjh9zm1VQIGUhhHBDyjAcu4umbUYXnbQ6jsvpTd+Ajw+qc0+ro1SQshBCuCXVrTfYy9FbN1gdxaUce0F9B206oYJDrY5TQcpCCOGeLmsGsfHodV9ZncS19u+GnCy3WgUFUhZCCDellEJddTXsSUMf9Z4D9HTqd2CzoTp2tzpKJVIWQgi3pa5IAh8b+uvPrI7iEto0HbvMtu2MCgq2Ok4lUhZCCLelwiJQnXui13+FLimxOk7t27cL8nNQ3a6yOslZpCyEEG5N9RkMxUXo9autjlLr9KZvwNcPdXk3q6OcRcpCCOHemreFxJboz95Hl5dbnabWaNOO3rwO2ndFBQRZHecsUhZCCLemlMK4ZjjkZKE3fm11nNqzJx1O5GG4wenIz0XKQgjh/tp3hcaJ6I/eQZeVWp2mVujUb8HPH9q73yookLIQQngApRTGH8dD9s/oLz+0Ok6N03Y7evN61OW/R/n7Wx3nnKQshBAeQbW+HDr2QH/6D3RuttVxatbu7VB4wu0OxDuTlIUQwmMYwyeANjHfWlCnLruqf1gP/oHQvovVUc5LykII4TFUbDxq2Fj4MRW9rm7sSqtN03Eq9nadUL5+Vsc5L5srXiQ7O5vk5GTy8/NRSpGUlMSQIUMqTaO1ZsmSJWzZsgV/f3+mTJlCYmKiK+IJITyI6nsNevN69Iq/oxNbouo3sjrSpTm0F07kut3pPX7LJUsWPj4+jBkzhhdeeIGnnnqKzz//nIyMjErTbNmyhWPHjjFv3jzuuOMOFi1a5IpoQggPowwD47b/Az8/zPlPoYs9+xTmeutGxxXx2ne1OsoFuaQsIiMjK5YSAgMDadiwIbm5uZWmSU1NpXfv3iilaNGiBUVFReTl5bkinhDCw6ioGIxJ0yHnZ8wFz3j07rR651ZIbOlWpyM/F5eshjpTVlYWBw4coFmzZpXuz83NJSYmpuL36OhocnNziYyMrDRdSkoKKSkpAMyaNavSPO7MZrN5TNZL4Q3j9IYxggeMM+YPnCp9mIKXHsd36UuEP/AUylb9jzQrx2meKuL4oX0EDxtNSC1nuNRxurQsTp8+zdy5cxk3bhxBQRd3OHtSUhJJSUkVv2dne8YudDExMR6T9VJ4wzi9YYzgIeNs1xV16x2UrHiN43MeQU2YhjJ8qvUUVo5Tp/0App1TjZpyupYznDnOBg0aVHt+l5VFeXk5c+fO5aqrrqJ797M35ERFRVV6w3JycoiKinJVPCGEhzL6DcU8VYz+51tgmjDhvotawrCC3pMGPj7QtJXVUZxyyTYLrTULFy6kYcOGDB069JzTdO3albVr16K1Zs+ePQQFBZ21CkoIIc7FuGY46o/j0Ju+wVw4C11y2upIVaJ3p8FlzVH+AVZHccol9bt7927Wrl1L48aNeeCBBwC49dZbK5Ykrr76ajp16sTmzZu555578PPzY8qUKa6IJoSoI4yBwzD9AtArXsV89kGMqQ+jouOsjnVeuuQ0HNqLuvpGq6NUiUvKolWrVrz77rsXnEYpxcSJE10RRwhRRxl9h6Bj4jD/PhfzifswxkxFdelldaxz27cT7HZUi3ZWJ6kSOYJbCFGnqPZdMR6eCzH1MBfOwlz8ArrA/XbD17vTwDCgWWuro1SJlIUQos5R9RpgTH8ONXQEetNazBmTMD9+B13kPgfw6T1p8LtmqIBAq6NUiUvkwd4AAA7aSURBVJSFEKJOUjYbxvWjMB5LhjaXoz9cjvmX8ZjLFqB3brP0qnu6pAQO/BfVsr1lGarLM/YvE0KIi6TqNcBnygx0xgF0yr/Q679Cr/0MgkKgRTuKOnRG10twfMt31V5J+3eBvdxjtleAlIUQwkuohCaocfeib70T0regt21E793Bya0bfplAQWw8NGiMavA7aNgY1fB3EJ+A8qnegX7O6N0/etT2CpCyEEJ4GeUfAJ17ojr3BCDK14ec1A3oQ3sh8zA68zB6+yYwTTQ4LnV6WXNU05aoxJaO8ziFXdoxYHpPGjRuigq8uDNZWEHKQgjh1YzwSNTl3VCX/3rta11WBj9noDMOwYE96P270V/8E223OyaIjkM1beUojsSW0KgJyuZbpdfTp4th/x5U0nW1MZxaI2UhhBC/oXx9IaEJKqEJ9OgDgC4tgcP7HcWxfxd6TzpsXOtY+rD5QsJlqNh4iImDmHqo6HoQHQsRUaiAM5Ygdm53bK9o19mKoV00KQshhKgC5ecPzVqjztjOoPNy4MBuR4Ec3o8++F/YvA7sdipd9DUgECKiIDwKTuQ6fveg7RUgZSGEEBdNRUZDZC9U51+PEtemHfJyIedndO5xyM+F/Fx0fo7jtjJQ/YZWebWVu5CyEEKIGqQMH8fqp+hYlNVhapAclCeEEMIpKQshhBBOSVkIIYRwSspCCCGEU1IWQgghnJKyEEII4ZSUhRBCCKekLIQQQjiltNba+WRCCCG8mSxZuMj06dOtjuAS3jBObxgjyDjrmksdp5SFEEIIp6QshBBCOOXz6KOPPmp1CG+RmJhodQSX8IZxesMYQcZZ11zKOGUDtxBCCKdkNZQQQginpCyEEEI4JWUhhBDCKblSnsUyMjJ49913CQ0NpX379vTo0cPqSLVi586dfPPNN5imSUZGBk8++aTVkWpFeno6K1euJCEhgSuuuIK2bdtaHalWZGRk8Omnn1JYWEj79u25+uqrrY5UK37++Wfef/99iouLuf/++62OU6OqOzYpi0uwYMECNm/eTHh4OHPnzq24f+vWrSxZsgTTNOnfvz833HDDeZ9jy5YtDB48mNatW/Pss8+6ZVnUxDhbt25N69at2bhxI02bNnVF7GqriXEqpQgICKCsrIzo6GhXxK62mhhnQkICd9xxB6ZpMn/+fLcsi5oYZ7169Zg8eXKl+d1ZdcZc3bFJWVyCPn36MGjQIJKTkyvuM02TxYsX89e//pXo6GgeeughunbtimmaLF++vNL8kydPpnfv3vzjH/8gNTWVkydPunoIVVIT4wwPDwfg22+/ZfLkyS7NX1U1Mc5WrVoxY8YM8vPzefPNN7nnnntcPQynaur9TE1N5YsvvqB3796uHkKV1OS/W09RnTEnJCRU67mlLC5BmzZtyMrKqnTf3r17iY+Pp169egD06tWLTZs2ceONN573cPuJEydimiZz5syp9cwXo6bGmZ2dTVBQEIGBgbWe+WLU1DgBQkJCKCsrq9W8F6umxtm1a1e6du3KM888w5VXXlnruaurJt9PT1GdMUtZWCw3N7fS6ofo6Gj++9//nnf6rKwsPvjgA0pKSrjuuutcEbFGVHecAF999RV9+/at7Wg1qrrj/P7779m2bRtFRUUMGjTIFRFrRHXHmZ6ezvfff095eTmdOnVyRcQaUd1xFhYWsmLFCg4ePMgHH3zAjTfe6IqYNep8Y67u2KQsLBYXF8edd95pdQyXGD58uNURal337t3p3r271TFqXdu2bevsxvszhYaGcscdd1gdo1ZUd2yy62wNi4qKIicnp+L3nJwcoqKiLExUO2ScdYuMs+6qqTFLWdSwpk2bcvToUbKysigvL2fdunV07drV6lg1TsZZt8g4666aGrOcG+oSvPjii+zYsYPCwkLCw8MZPnw4/fr1Y/PmzbzxxhuYpknfvn0ZNmyY1VEviYxTxumJvGWcZ6rNMUtZCCGEcEpWQwkhhHBKykIIIYRTUhZCCCGckrIQQgjhlJSFEEIIp6QshBBCOCVlITzOzp07uffee62OUePWrFnDiBEjGDNmDBkZGQC8++67zJs3z+JkVTN8+HCOHTvmdLrMzEzGjBnDiBEjWL16tQuSiZogZSGqZerUqWzfvt3SDK1bt+all16qled+9NFHGTVqFGPGjOG2225jzpw55OXlVWne9PR0Jk2adEmv36JFC5YtW1btM4J6kgYNGrBs2TJat25tdRRRDVIWwu2Ypmnp60+YMIFly5Yxb948Tp8+zbJlyyzNI4Q7kLPOihphmib/+te/WL16NUVFRbRr14477riDkJAQAJ5//nl27txJaWkpl112GRMnTqRRo0YAJCcn4+fnR3Z2Njt27OCBBx7g1VdfZeDAgaxdu5bjx4/TsWNHpk6dip+fH+np6bz88sssXLgQcCztnG9agA8//JBPPvkEpRTDhw/n1VdfZd68ecTHx19wTMHBwXTr1o3PP/+84r7//Oc//Otf/yInJ4ewsDCuv/56BgwYwOnTp3n66acpLy9nzJgxALz00ktERERc8O9SXampqSxfvpzc3NyKv+P/lkL279/PwoULOXbsGB07dkQpRf369bnlllvOep5jx47xyiuvcPDgQWw2G+3ateO+++4D4MiRIyxdupT9+/djs9kYPHgww4YNY+/evSxZsoSffvoJPz8/unfvztixY7HZzv4YKSsrY8WKFaxfv57y8nK6devGuHHjKt4T4XlkyULUiM8++4xNmzbx6KOP8uqrrxISEsKiRYsqHu/YsSPz5s1j0aJFNGnS5Kz18N9++y033ngjb7zxBq1atQJg/fr1zJgxg+TkZA4fPsyaNWvO+/rnm3br1q18/PHHPPLII8ybN4/09PQqj6mwsJCNGzdWKpXw8HAefPBB3njjDaZMmcIbb7zB/v37CQgIYMaMGURGRrJs2TKWLVtGVFSU079LdWRmZvLSSy8xbtw4Fi1aRKdOnXj22WcpLy+nvLycOXPm0KdPH15//XWuuOIKNm7ceN7neuedd7j88stZsmQJr7zyCoMHDwbg1KlTPPHEE3Ts2LGiVNu3bw+AYRiMHTuWxYsX8+STT5KWllapSM/09ttvc/ToUWbPns28efPIzc1l1apVFzVu4R6kLESN+PLLL7nllluIjo7G19eXm2++me+//x673Q5Av379CAwMrHjs0KFDFBcXV8zfrVs3WrVqhWEYFd8+Bw8eTFRUFCEhIXTp0oWDBw+e9/XPN+26devo27cvjRo1wt/fv0rX1FiyZAljx47ltttuo6CggAkTJlQ81rlzZ+Lj41FK0aZNGzp06MCuXbsu+u9SHevWraNTp0506NABm83GtddeS2lpKbt372bPnj3Y7XYGDx6MzWaje/fuNGvW7LzPZbPZOH78OHl5efj5+VUU9A8//EBERATXXnstfn5+BAYG0rx5cwASExNp0aIFPj4+xMXFkZSUxI4dO856bq01q1evZuzYsYSEhBAYGMiwYcP47rvvqj1m4T5kNZSoEcePH2fOnDkopSruMwyDEydOEBERwYoVK9iwYQMFBQUV0xQUFBAUFARQ6Upe/xMREVFx28/Pj9zc3PO+/vmmzcvLo2nTphWPnet1fmv8+PH079+fw4cPM2vWLHJycoiJiQFgy5YtrFq1iszMTLTWlJSU0Lhx4/M+14X+LtW9pkBeXh6xsbGVnicmJobc3FwMwyAqKqrS61xorKNHj+add95hxowZBAcHM3ToUPr160dOTk7F5Td/KzMzkzfffJN9+/ZRWlqK3W4nMTHxrOkKCgooKSmpdJlSrbXl26LEpZGyEDUiOjqayZMnV3xDPdPatWtJTU3lkUceITY2luLiYsaPH19pmjM/5GpSZGTkWRd+qarGjRszbNgwFi9eXLG6Z+7cudx111107doVm83Gc889VzH9ucZwob9LdUVGRnL48OGK37XWZGdnV5REbm4uWuuKHDk5OefdLhMREVGx59auXbt44oknaNOmDdHR0axbt+6c8yxatIjLLruMe++9l8DAQD755BM2bNhw1nShoaH4+fnx/PPP1/kLC3kTWQ0lqs1ut1NaWlrxY7fbGTBgAO+88w7Hjx8HHN8uN23aBDjWg9tsNkJCQigpKWHFihUuy9qzZ0/WrFlDRkYGJSUl1V5v3qdPH06cOEFqairl5eWUlZURFhaGj48PW7ZsqbQbcXh4OIWFhZVWr13o71JdvXr1YsuWLfz444+Ul5fz0Ucf4evrS8uWLWnRogWGYfDZZ59ht9vZtGkTe/fuPe9zrV+/vqI4g4ODAUfZdenShby8PD755BPKyso4depUxTWqT506RVBQEAEBAfz000988cUX53xuwzDo378/S5cu5cSJE4DjOtBbt269qHEL9yBLFqLannnmmUq/Dxs2rGJbwJNPPkleXh7h4eH07NmTbt268Yc//IFt27YxadIkQkJCGDFixHk/aGpap06dGDx4MI899hiGYXDTTTexdu3ac+7Bcy7/2xvovffeo1u3bowfP54XXniBsrIyunTpUumKYw0bNuSKK67grrvuwjRNnn/+eYYMGQKc++9SXQ0aNODuu+/m9ddfr9gb6sEHH6wYy5///GcWLlzI8uXL6dSpE126dDnvOPft28fSpUspLi4mIiKC8ePHV6x++utf/8rSpUtZtWoVNpuNa665hubNmzNmzBhee+01PvzwQ5o0aUKvXr1IS0s75/OPGjWKVatW8fDDD1NYWEhUVBQDBgygY8eO1R63cA9y8SPhVTIyMrj//vtZvnw5Pj4+VsepZO3atbz22mvYbDaefPLJSz4wb8aMGQwYMIC+ffvWUMKacfToUR566CHKy8uZOHEiffr0sTqSqAIpC1Hnbdy4kU6dOlFSUkJycjJKKf7yl79YHavG7dixgwYNGhAaGso333zD3//+d+bPn09kZKTV0UQdIKuhRJ335ZdfkpycjGEYtGnThokTJ1odqVZkZmbywgsvcPr0aerVq8f9998vRSFqjCxZCCGEcEr2hhJCCOGUlIUQQginpCyEEEI4JWUhhBDCKSkLIYQQTklZCCGEcOr/AXYak4krAqtvAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"L1LNyRTfrI1h","colab_type":"code","colab":{}},"source":["# step decay learning rate scheduler\n","def lr_sched(epoch):\n","    lr = init_lr\n","\n","    if epoch < 1:\n","        lr = init_lr / 10\n","    elif epoch < 90:\n","        lr = init_lr\n","    elif epoch < 135:\n","        lr = init_lr / 10\n","    else:\n","        lr = init_lr / 100\n","    \n","    return lr\n","\n","# # cosine decay learning rate scheduler\n","# class CosineScheduler(Callback):\n","#     def __init__(self, max_lr, steps_per_epoch, tot_epochs, warmup = 5):\n","#         # parent class constructor\n","#         super(CosineScheduler, self).__init__()\n","\n","#         # initialize the instance variables\n","#         self.max_lr = max_lr\n","#         self.warm_steps = steps_per_epoch * warmup\n","#         self.reg_steps = steps_per_epoch * (tot_epochs - warmup)\n","#         self.history = {\"lrs\" : []}\n","    \n","#     def on_train_begin(self, logs = None):\n","#         # initialize a counter to keep track of the number of batches seen\n","#         self.iterations = 0\n","    \n","#     def on_batch_begin(self, batch, logs = None):\n","#         # increment the number of iterations\n","#         self.iterations += 1\n","\n","#         # calculate the learning rate\n","#         if self.iterations <= self.warm_steps:\n","#             lr = (self.iterations / self.warm_steps) * self.max_lr\n","#         else:\n","#             lr = (self.max_lr / 2.0) * (1 + np.cos(((self.iterations - self.warm_steps) / self.reg_steps) * np.pi))\n","        \n","#         # update the learning rate\n","#         K.set_value(self.model.optimizer.lr, lr)\n","\n","#         # add the current learning rate to the history dictionary\n","#         self.history[\"lrs\"].append(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvray3Prsi6P","colab_type":"code","colab":{}},"source":["# initialize the callbacks\n","mc = ModelCheckpoint(os.path.sep.join([\"models\", model_name + \"_{epoch:03d}.h5\"]))\n","tm = TrainingMonitor(f\"{model_name}.png\", f\"{model_name}.json\")\n","lr = LearningRateScheduler(lr_sched)\n","# cs = CosineScheduler(init_lr, steps_per_epoch, epochs)\n","callbacks = [mc, tm, lr]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nStBph8FnJJ3","colab_type":"code","colab":{}},"source":["# initialize the model and compile it\n","model = MODELS[model_name]\n","opt = SGD(lr = init_lr, momentum = 0.9)\n","# loss = CategoricalCrossentropy(label_smoothing = 0.1)\n","loss = CategoricalCrossentropy()\n","model.compile(optimizer = opt, loss = loss, metrics = [\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nN67f8DPvW5t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592925314450,"user_tz":-330,"elapsed":89246,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"d07f5e28-7bee-4076-dd03-c6d482b27289"},"source":["# train the model\n","model.fit_generator(train_datagen, steps_per_epoch = steps_per_epoch, epochs = epochs,\n","                    validation_data = val_datagen, validation_steps = validation_steps,\n","                    callbacks = callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-22-f6e0bb247238>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/180\n","352/352 [==============================] - 25s 70ms/step - loss: 1.8355 - accuracy: 0.3941 - val_loss: 1.5371 - val_accuracy: 0.4888 - lr: 0.0100\n","Epoch 2/180\n","352/352 [==============================] - 24s 69ms/step - loss: 1.4631 - accuracy: 0.5276 - val_loss: 1.1503 - val_accuracy: 0.6460 - lr: 0.1000\n","Epoch 3/180\n","352/352 [==============================] - 24s 69ms/step - loss: 1.0839 - accuracy: 0.6761 - val_loss: 1.0522 - val_accuracy: 0.6844 - lr: 0.1000\n","Epoch 4/180\n","352/352 [==============================] - 24s 69ms/step - loss: 0.9218 - accuracy: 0.7401 - val_loss: 1.1958 - val_accuracy: 0.6694 - lr: 0.1000\n","Epoch 5/180\n","352/352 [==============================] - 24s 69ms/step - loss: 0.8299 - accuracy: 0.7721 - val_loss: 1.0223 - val_accuracy: 0.7194 - lr: 0.1000\n","Epoch 6/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.7678 - accuracy: 0.7958 - val_loss: 0.8337 - val_accuracy: 0.7692 - lr: 0.1000\n","Epoch 7/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.7337 - accuracy: 0.8076 - val_loss: 0.8367 - val_accuracy: 0.7772 - lr: 0.1000\n","Epoch 8/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.6968 - accuracy: 0.8194 - val_loss: 0.6808 - val_accuracy: 0.8202 - lr: 0.1000\n","Epoch 9/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.6724 - accuracy: 0.8308 - val_loss: 0.7138 - val_accuracy: 0.8192 - lr: 0.1000\n","Epoch 10/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.6495 - accuracy: 0.8375 - val_loss: 0.6719 - val_accuracy: 0.8314 - lr: 0.1000\n","Epoch 11/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.6257 - accuracy: 0.8467 - val_loss: 0.6829 - val_accuracy: 0.8278 - lr: 0.1000\n","Epoch 12/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.6099 - accuracy: 0.8531 - val_loss: 0.6868 - val_accuracy: 0.8326 - lr: 0.1000\n","Epoch 13/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5989 - accuracy: 0.8551 - val_loss: 0.6477 - val_accuracy: 0.8448 - lr: 0.1000\n","Epoch 14/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5863 - accuracy: 0.8612 - val_loss: 0.6478 - val_accuracy: 0.8456 - lr: 0.1000\n","Epoch 15/180\n","352/352 [==============================] - 24s 69ms/step - loss: 0.5795 - accuracy: 0.8635 - val_loss: 0.6136 - val_accuracy: 0.8496 - lr: 0.1000\n","Epoch 16/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5721 - accuracy: 0.8663 - val_loss: 0.6068 - val_accuracy: 0.8582 - lr: 0.1000\n","Epoch 17/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5587 - accuracy: 0.8756 - val_loss: 0.6812 - val_accuracy: 0.8388 - lr: 0.1000\n","Epoch 18/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5540 - accuracy: 0.8771 - val_loss: 0.6421 - val_accuracy: 0.8490 - lr: 0.1000\n","Epoch 19/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5452 - accuracy: 0.8791 - val_loss: 0.5839 - val_accuracy: 0.8642 - lr: 0.1000\n","Epoch 20/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5359 - accuracy: 0.8818 - val_loss: 0.6737 - val_accuracy: 0.8434 - lr: 0.1000\n","Epoch 21/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5376 - accuracy: 0.8835 - val_loss: 0.6540 - val_accuracy: 0.8550 - lr: 0.1000\n","Epoch 22/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5276 - accuracy: 0.8872 - val_loss: 0.7124 - val_accuracy: 0.8274 - lr: 0.1000\n","Epoch 23/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5318 - accuracy: 0.8852 - val_loss: 0.6518 - val_accuracy: 0.8564 - lr: 0.1000\n","Epoch 24/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5217 - accuracy: 0.8907 - val_loss: 0.6546 - val_accuracy: 0.8576 - lr: 0.1000\n","Epoch 25/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5208 - accuracy: 0.8905 - val_loss: 0.6589 - val_accuracy: 0.8564 - lr: 0.1000\n","Epoch 26/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5180 - accuracy: 0.8934 - val_loss: 0.6197 - val_accuracy: 0.8710 - lr: 0.1000\n","Epoch 27/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5173 - accuracy: 0.8932 - val_loss: 0.6241 - val_accuracy: 0.8676 - lr: 0.1000\n","Epoch 28/180\n","352/352 [==============================] - 24s 69ms/step - loss: 0.5074 - accuracy: 0.8976 - val_loss: 0.6604 - val_accuracy: 0.8498 - lr: 0.1000\n","Epoch 29/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5105 - accuracy: 0.8977 - val_loss: 0.6820 - val_accuracy: 0.8506 - lr: 0.1000\n","Epoch 30/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5113 - accuracy: 0.8998 - val_loss: 0.6493 - val_accuracy: 0.8604 - lr: 0.1000\n","Epoch 31/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5076 - accuracy: 0.8982 - val_loss: 0.6801 - val_accuracy: 0.8536 - lr: 0.1000\n","Epoch 32/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5083 - accuracy: 0.8989 - val_loss: 0.6289 - val_accuracy: 0.8694 - lr: 0.1000\n","Epoch 33/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5044 - accuracy: 0.9012 - val_loss: 0.6043 - val_accuracy: 0.8654 - lr: 0.1000\n","Epoch 34/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4974 - accuracy: 0.9035 - val_loss: 0.6572 - val_accuracy: 0.8622 - lr: 0.1000\n","Epoch 35/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4998 - accuracy: 0.9039 - val_loss: 0.6425 - val_accuracy: 0.8614 - lr: 0.1000\n","Epoch 36/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4998 - accuracy: 0.9028 - val_loss: 0.6264 - val_accuracy: 0.8632 - lr: 0.1000\n","Epoch 37/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5046 - accuracy: 0.9019 - val_loss: 0.6922 - val_accuracy: 0.8492 - lr: 0.1000\n","Epoch 38/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4987 - accuracy: 0.9044 - val_loss: 0.6096 - val_accuracy: 0.8830 - lr: 0.1000\n","Epoch 39/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4997 - accuracy: 0.9052 - val_loss: 0.6352 - val_accuracy: 0.8668 - lr: 0.1000\n","Epoch 40/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4976 - accuracy: 0.9039 - val_loss: 0.7165 - val_accuracy: 0.8490 - lr: 0.1000\n","Epoch 41/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.5019 - accuracy: 0.9052 - val_loss: 0.6577 - val_accuracy: 0.8674 - lr: 0.1000\n","Epoch 42/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4967 - accuracy: 0.9069 - val_loss: 0.5989 - val_accuracy: 0.8792 - lr: 0.1000\n","Epoch 43/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4929 - accuracy: 0.9087 - val_loss: 0.7040 - val_accuracy: 0.8476 - lr: 0.1000\n","Epoch 44/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4859 - accuracy: 0.9112 - val_loss: 0.6365 - val_accuracy: 0.8666 - lr: 0.1000\n","Epoch 45/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4895 - accuracy: 0.9102 - val_loss: 0.6856 - val_accuracy: 0.8542 - lr: 0.1000\n","Epoch 46/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4913 - accuracy: 0.9091 - val_loss: 0.6523 - val_accuracy: 0.8692 - lr: 0.1000\n","Epoch 47/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4880 - accuracy: 0.9108 - val_loss: 0.6182 - val_accuracy: 0.8728 - lr: 0.1000\n","Epoch 48/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4898 - accuracy: 0.9107 - val_loss: 0.5805 - val_accuracy: 0.8916 - lr: 0.1000\n","Epoch 49/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4900 - accuracy: 0.9113 - val_loss: 0.6115 - val_accuracy: 0.8802 - lr: 0.1000\n","Epoch 50/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4816 - accuracy: 0.9132 - val_loss: 0.6135 - val_accuracy: 0.8826 - lr: 0.1000\n","Epoch 51/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4823 - accuracy: 0.9137 - val_loss: 0.6286 - val_accuracy: 0.8756 - lr: 0.1000\n","Epoch 52/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4816 - accuracy: 0.9137 - val_loss: 0.6812 - val_accuracy: 0.8588 - lr: 0.1000\n","Epoch 53/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4881 - accuracy: 0.9125 - val_loss: 0.6405 - val_accuracy: 0.8716 - lr: 0.1000\n","Epoch 54/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4841 - accuracy: 0.9150 - val_loss: 0.6326 - val_accuracy: 0.8724 - lr: 0.1000\n","Epoch 55/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4834 - accuracy: 0.9143 - val_loss: 0.6507 - val_accuracy: 0.8766 - lr: 0.1000\n","Epoch 56/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4820 - accuracy: 0.9152 - val_loss: 0.6972 - val_accuracy: 0.8632 - lr: 0.1000\n","Epoch 57/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4886 - accuracy: 0.9124 - val_loss: 0.6543 - val_accuracy: 0.8694 - lr: 0.1000\n","Epoch 58/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4814 - accuracy: 0.9159 - val_loss: 0.5998 - val_accuracy: 0.8868 - lr: 0.1000\n","Epoch 59/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4786 - accuracy: 0.9159 - val_loss: 0.6580 - val_accuracy: 0.8732 - lr: 0.1000\n","Epoch 60/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4730 - accuracy: 0.9180 - val_loss: 0.6562 - val_accuracy: 0.8674 - lr: 0.1000\n","Epoch 61/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4835 - accuracy: 0.9154 - val_loss: 0.5919 - val_accuracy: 0.8912 - lr: 0.1000\n","Epoch 62/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4755 - accuracy: 0.9170 - val_loss: 0.6285 - val_accuracy: 0.8766 - lr: 0.1000\n","Epoch 63/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4771 - accuracy: 0.9166 - val_loss: 0.6636 - val_accuracy: 0.8580 - lr: 0.1000\n","Epoch 64/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4796 - accuracy: 0.9166 - val_loss: 0.6680 - val_accuracy: 0.8624 - lr: 0.1000\n","Epoch 65/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4774 - accuracy: 0.9163 - val_loss: 0.8110 - val_accuracy: 0.8368 - lr: 0.1000\n","Epoch 66/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4784 - accuracy: 0.9164 - val_loss: 0.7199 - val_accuracy: 0.8592 - lr: 0.1000\n","Epoch 67/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4724 - accuracy: 0.9194 - val_loss: 0.6423 - val_accuracy: 0.8692 - lr: 0.1000\n","Epoch 68/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4715 - accuracy: 0.9196 - val_loss: 0.6716 - val_accuracy: 0.8658 - lr: 0.1000\n","Epoch 69/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.4774 - accuracy: 0.9182 - val_loss: 0.6395 - val_accuracy: 0.8810 - lr: 0.1000\n","Epoch 70/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4783 - accuracy: 0.9181 - val_loss: 0.6107 - val_accuracy: 0.8850 - lr: 0.1000\n","Epoch 71/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4756 - accuracy: 0.9186 - val_loss: 0.5834 - val_accuracy: 0.8870 - lr: 0.1000\n","Epoch 72/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4742 - accuracy: 0.9200 - val_loss: 0.6624 - val_accuracy: 0.8650 - lr: 0.1000\n","Epoch 73/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4729 - accuracy: 0.9200 - val_loss: 0.6241 - val_accuracy: 0.8850 - lr: 0.1000\n","Epoch 74/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4729 - accuracy: 0.9193 - val_loss: 0.7390 - val_accuracy: 0.8536 - lr: 0.1000\n","Epoch 75/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4731 - accuracy: 0.9191 - val_loss: 0.6202 - val_accuracy: 0.8790 - lr: 0.1000\n","Epoch 76/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4723 - accuracy: 0.9202 - val_loss: 0.5942 - val_accuracy: 0.8844 - lr: 0.1000\n","Epoch 77/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4704 - accuracy: 0.9210 - val_loss: 0.6553 - val_accuracy: 0.8718 - lr: 0.1000\n","Epoch 78/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4705 - accuracy: 0.9216 - val_loss: 0.6784 - val_accuracy: 0.8696 - lr: 0.1000\n","Epoch 79/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4724 - accuracy: 0.9205 - val_loss: 0.6352 - val_accuracy: 0.8772 - lr: 0.1000\n","Epoch 80/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.4704 - accuracy: 0.9209 - val_loss: 0.6034 - val_accuracy: 0.8878 - lr: 0.1000\n","Epoch 81/180\n","352/352 [==============================] - 27s 76ms/step - loss: 0.4678 - accuracy: 0.9206 - val_loss: 0.6694 - val_accuracy: 0.8698 - lr: 0.1000\n","Epoch 82/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4734 - accuracy: 0.9193 - val_loss: 0.6358 - val_accuracy: 0.8740 - lr: 0.1000\n","Epoch 83/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4731 - accuracy: 0.9205 - val_loss: 0.6331 - val_accuracy: 0.8764 - lr: 0.1000\n","Epoch 84/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4750 - accuracy: 0.9200 - val_loss: 0.6443 - val_accuracy: 0.8712 - lr: 0.1000\n","Epoch 85/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4644 - accuracy: 0.9231 - val_loss: 0.6275 - val_accuracy: 0.8804 - lr: 0.1000\n","Epoch 86/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4687 - accuracy: 0.9225 - val_loss: 0.6773 - val_accuracy: 0.8672 - lr: 0.1000\n","Epoch 87/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4610 - accuracy: 0.9244 - val_loss: 0.6366 - val_accuracy: 0.8808 - lr: 0.1000\n","Epoch 88/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4604 - accuracy: 0.9238 - val_loss: 0.6099 - val_accuracy: 0.8804 - lr: 0.1000\n","Epoch 89/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.4692 - accuracy: 0.9205 - val_loss: 0.6038 - val_accuracy: 0.8850 - lr: 0.1000\n","Epoch 90/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.4628 - accuracy: 0.9243 - val_loss: 0.6277 - val_accuracy: 0.8810 - lr: 0.1000\n","Epoch 91/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.3846 - accuracy: 0.9511 - val_loss: 0.5016 - val_accuracy: 0.9166 - lr: 0.0100\n","Epoch 92/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.3371 - accuracy: 0.9668 - val_loss: 0.5146 - val_accuracy: 0.9196 - lr: 0.0100\n","Epoch 93/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.3198 - accuracy: 0.9716 - val_loss: 0.4904 - val_accuracy: 0.9228 - lr: 0.0100\n","Epoch 94/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.3078 - accuracy: 0.9750 - val_loss: 0.5146 - val_accuracy: 0.9218 - lr: 0.0100\n","Epoch 95/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.2969 - accuracy: 0.9778 - val_loss: 0.4937 - val_accuracy: 0.9210 - lr: 0.0100\n","Epoch 96/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.2900 - accuracy: 0.9789 - val_loss: 0.5172 - val_accuracy: 0.9190 - lr: 0.0100\n","Epoch 97/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2820 - accuracy: 0.9808 - val_loss: 0.5019 - val_accuracy: 0.9220 - lr: 0.0100\n","Epoch 98/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2752 - accuracy: 0.9821 - val_loss: 0.4913 - val_accuracy: 0.9248 - lr: 0.0100\n","Epoch 99/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.2688 - accuracy: 0.9837 - val_loss: 0.5144 - val_accuracy: 0.9198 - lr: 0.0100\n","Epoch 100/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2641 - accuracy: 0.9847 - val_loss: 0.4889 - val_accuracy: 0.9252 - lr: 0.0100\n","Epoch 101/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2582 - accuracy: 0.9850 - val_loss: 0.5014 - val_accuracy: 0.9212 - lr: 0.0100\n","Epoch 102/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2518 - accuracy: 0.9870 - val_loss: 0.5137 - val_accuracy: 0.9224 - lr: 0.0100\n","Epoch 103/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2496 - accuracy: 0.9866 - val_loss: 0.5062 - val_accuracy: 0.9226 - lr: 0.0100\n","Epoch 104/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.2450 - accuracy: 0.9875 - val_loss: 0.5005 - val_accuracy: 0.9220 - lr: 0.0100\n","Epoch 105/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.2386 - accuracy: 0.9884 - val_loss: 0.5087 - val_accuracy: 0.9218 - lr: 0.0100\n","Epoch 106/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2353 - accuracy: 0.9886 - val_loss: 0.5040 - val_accuracy: 0.9204 - lr: 0.0100\n","Epoch 107/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2309 - accuracy: 0.9903 - val_loss: 0.5084 - val_accuracy: 0.9246 - lr: 0.0100\n","Epoch 108/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.2282 - accuracy: 0.9894 - val_loss: 0.5015 - val_accuracy: 0.9226 - lr: 0.0100\n","Epoch 109/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2254 - accuracy: 0.9896 - val_loss: 0.5145 - val_accuracy: 0.9218 - lr: 0.0100\n","Epoch 110/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2208 - accuracy: 0.9905 - val_loss: 0.4962 - val_accuracy: 0.9256 - lr: 0.0100\n","Epoch 111/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.2178 - accuracy: 0.9910 - val_loss: 0.5044 - val_accuracy: 0.9270 - lr: 0.0100\n","Epoch 112/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2128 - accuracy: 0.9924 - val_loss: 0.5104 - val_accuracy: 0.9238 - lr: 0.0100\n","Epoch 113/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2119 - accuracy: 0.9918 - val_loss: 0.5072 - val_accuracy: 0.9222 - lr: 0.0100\n","Epoch 114/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.2079 - accuracy: 0.9921 - val_loss: 0.5196 - val_accuracy: 0.9248 - lr: 0.0100\n","Epoch 115/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.2051 - accuracy: 0.9921 - val_loss: 0.4870 - val_accuracy: 0.9254 - lr: 0.0100\n","Epoch 116/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.2020 - accuracy: 0.9928 - val_loss: 0.5179 - val_accuracy: 0.9244 - lr: 0.0100\n","Epoch 117/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1976 - accuracy: 0.9934 - val_loss: 0.5133 - val_accuracy: 0.9238 - lr: 0.0100\n","Epoch 118/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1950 - accuracy: 0.9939 - val_loss: 0.4986 - val_accuracy: 0.9238 - lr: 0.0100\n","Epoch 119/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1930 - accuracy: 0.9935 - val_loss: 0.5147 - val_accuracy: 0.9226 - lr: 0.0100\n","Epoch 120/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1916 - accuracy: 0.9934 - val_loss: 0.5066 - val_accuracy: 0.9226 - lr: 0.0100\n","Epoch 121/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1872 - accuracy: 0.9944 - val_loss: 0.5090 - val_accuracy: 0.9248 - lr: 0.0100\n","Epoch 122/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1872 - accuracy: 0.9932 - val_loss: 0.5266 - val_accuracy: 0.9230 - lr: 0.0100\n","Epoch 123/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1826 - accuracy: 0.9941 - val_loss: 0.5051 - val_accuracy: 0.9266 - lr: 0.0100\n","Epoch 124/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1804 - accuracy: 0.9943 - val_loss: 0.5095 - val_accuracy: 0.9230 - lr: 0.0100\n","Epoch 125/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1779 - accuracy: 0.9944 - val_loss: 0.5150 - val_accuracy: 0.9224 - lr: 0.0100\n","Epoch 126/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1766 - accuracy: 0.9941 - val_loss: 0.5149 - val_accuracy: 0.9272 - lr: 0.0100\n","Epoch 127/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1743 - accuracy: 0.9942 - val_loss: 0.5135 - val_accuracy: 0.9206 - lr: 0.0100\n","Epoch 128/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1732 - accuracy: 0.9941 - val_loss: 0.5198 - val_accuracy: 0.9168 - lr: 0.0100\n","Epoch 129/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1705 - accuracy: 0.9944 - val_loss: 0.4808 - val_accuracy: 0.9292 - lr: 0.0100\n","Epoch 130/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1685 - accuracy: 0.9946 - val_loss: 0.5117 - val_accuracy: 0.9250 - lr: 0.0100\n","Epoch 131/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1650 - accuracy: 0.9953 - val_loss: 0.5057 - val_accuracy: 0.9224 - lr: 0.0100\n","Epoch 132/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1640 - accuracy: 0.9948 - val_loss: 0.5087 - val_accuracy: 0.9200 - lr: 0.0100\n","Epoch 133/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.1631 - accuracy: 0.9943 - val_loss: 0.4915 - val_accuracy: 0.9262 - lr: 0.0100\n","Epoch 134/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1583 - accuracy: 0.9957 - val_loss: 0.5073 - val_accuracy: 0.9242 - lr: 0.0100\n","Epoch 135/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1590 - accuracy: 0.9946 - val_loss: 0.5005 - val_accuracy: 0.9222 - lr: 0.0100\n","Epoch 136/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1553 - accuracy: 0.9960 - val_loss: 0.5159 - val_accuracy: 0.9212 - lr: 0.0010\n","Epoch 137/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1536 - accuracy: 0.9967 - val_loss: 0.4734 - val_accuracy: 0.9262 - lr: 0.0010\n","Epoch 138/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1527 - accuracy: 0.9965 - val_loss: 0.4839 - val_accuracy: 0.9232 - lr: 0.0010\n","Epoch 139/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1525 - accuracy: 0.9968 - val_loss: 0.4972 - val_accuracy: 0.9206 - lr: 0.0010\n","Epoch 140/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1517 - accuracy: 0.9970 - val_loss: 0.5059 - val_accuracy: 0.9244 - lr: 0.0010\n","Epoch 141/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1509 - accuracy: 0.9975 - val_loss: 0.4711 - val_accuracy: 0.9254 - lr: 0.0010\n","Epoch 142/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1503 - accuracy: 0.9975 - val_loss: 0.4966 - val_accuracy: 0.9234 - lr: 0.0010\n","Epoch 143/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1495 - accuracy: 0.9978 - val_loss: 0.4989 - val_accuracy: 0.9248 - lr: 0.0010\n","Epoch 144/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1501 - accuracy: 0.9973 - val_loss: 0.4685 - val_accuracy: 0.9256 - lr: 0.0010\n","Epoch 145/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1498 - accuracy: 0.9973 - val_loss: 0.5097 - val_accuracy: 0.9216 - lr: 0.0010\n","Epoch 146/180\n","352/352 [==============================] - 24s 68ms/step - loss: 0.1485 - accuracy: 0.9980 - val_loss: 0.5130 - val_accuracy: 0.9232 - lr: 0.0010\n","Epoch 147/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1487 - accuracy: 0.9977 - val_loss: 0.4815 - val_accuracy: 0.9248 - lr: 0.0010\n","Epoch 148/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1485 - accuracy: 0.9977 - val_loss: 0.4691 - val_accuracy: 0.9274 - lr: 0.0010\n","Epoch 149/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1474 - accuracy: 0.9980 - val_loss: 0.5219 - val_accuracy: 0.9226 - lr: 0.0010\n","Epoch 150/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1472 - accuracy: 0.9979 - val_loss: 0.4770 - val_accuracy: 0.9260 - lr: 0.0010\n","Epoch 151/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1473 - accuracy: 0.9978 - val_loss: 0.4939 - val_accuracy: 0.9244 - lr: 0.0010\n","Epoch 152/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1466 - accuracy: 0.9982 - val_loss: 0.4784 - val_accuracy: 0.9294 - lr: 0.0010\n","Epoch 153/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1466 - accuracy: 0.9979 - val_loss: 0.4911 - val_accuracy: 0.9276 - lr: 0.0010\n","Epoch 154/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1460 - accuracy: 0.9984 - val_loss: 0.4929 - val_accuracy: 0.9252 - lr: 0.0010\n","Epoch 155/180\n","352/352 [==============================] - 24s 67ms/step - loss: 0.1461 - accuracy: 0.9980 - val_loss: 0.4968 - val_accuracy: 0.9250 - lr: 0.0010\n","Epoch 156/180\n","352/352 [==============================] - 23s 67ms/step - loss: 0.1459 - accuracy: 0.9980 - val_loss: 0.4910 - val_accuracy: 0.9244 - lr: 0.0010\n","Epoch 157/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1461 - accuracy: 0.9978 - val_loss: 0.5003 - val_accuracy: 0.9266 - lr: 0.0010\n","Epoch 158/180\n","352/352 [==============================] - 23s 65ms/step - loss: 0.1452 - accuracy: 0.9980 - val_loss: 0.4877 - val_accuracy: 0.9252 - lr: 0.0010\n","Epoch 159/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1454 - accuracy: 0.9981 - val_loss: 0.4822 - val_accuracy: 0.9270 - lr: 0.0010\n","Epoch 160/180\n","352/352 [==============================] - 23s 66ms/step - loss: 0.1444 - accuracy: 0.9985 - val_loss: 0.4963 - val_accuracy: 0.9264 - lr: 0.0010\n","Epoch 161/180\n","352/352 [==============================] - 23s 65ms/step - loss: 0.1452 - accuracy: 0.9978 - val_loss: 0.4925 - val_accuracy: 0.9248 - lr: 0.0010\n","Epoch 162/180\n","352/352 [==============================] - 23s 65ms/step - loss: 0.1443 - accuracy: 0.9982 - val_loss: 0.4855 - val_accuracy: 0.9250 - lr: 0.0010\n","Epoch 163/180\n","352/352 [==============================] - 23s 65ms/step - loss: 0.1444 - accuracy: 0.9980 - val_loss: 0.4983 - val_accuracy: 0.9234 - lr: 0.0010\n","Epoch 164/180\n","352/352 [==============================] - 23s 65ms/step - loss: 0.1447 - accuracy: 0.9978 - val_loss: 0.4885 - val_accuracy: 0.9254 - lr: 0.0010\n","Epoch 165/180\n","165/352 [=============>................] - ETA: 11s - loss: 0.1436 - accuracy: 0.9985Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2mR2W2_lC0fC","colab_type":"text"},"source":["# INFERENCE #"]},{"cell_type":"code","metadata":{"id":"8fQggWD1C2Nt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1592925941480,"user_tz":-330,"elapsed":15354,"user":{"displayName":"Varun Anand","photoUrl":"","userId":"08614398225445890360"}},"outputId":"ea0c8d1f-ba85-432b-b994-e530656b4677"},"source":["# load the trained model\n","test_model = load_model(f\"models/{model_name}_{epochs}.h5\", custom_objects = {\"Downsample\" : Downsample, \"Mish\" : Mish})\n","\n","# initialize the data generator\n","test_gen = CifarGenerator(x_test, y_test, bs, preprocessors = [mp, iap]).generator()\n","\n","# evaluate the model\n","H = test_model.evaluate_generator(test_gen, steps = test_steps)\n","print(H)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-24-6c27c770f088>:8: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.evaluate, which supports generators.\n","[0.5172573924064636, 0.921500027179718]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7yXJ9X82dZa2","colab_type":"text"},"source":["# INSPECTION #"]},{"cell_type":"code","metadata":{"id":"3qSSdY5xdQn6","colab_type":"code","colab":{}},"source":["def get_layer_index(model, name):\n","    for i, layer in enumerate(model.layers):\n","        if(layer.name == name):\n","            return i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPXuHu9TdXNF","colab_type":"code","colab":{}},"source":["def calc_cosine_similarity(x):\n","    x = np.reshape(x, (x.shape[-1], -1))\n","    norms = np.linalg.norm(x, axis = -1, keepdims = True)\n","    xn = x / (norms + 1e-6)\n","    grad = np.matmul(xn, np.transpose(xn))\n","    grad = (180 / np.pi) * np.arccos(grad)\n","    \n","    grad = grad * (1 - np.eye(grad.shape[1], dtype = grad.dtype))\n","\n","    grad = (1 / (grad.shape[0] - 1)) * np.sum(grad, axis = -1)    \n","\n","    return np.min(grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSnoxXpMdeXY","colab_type":"code","colab":{}},"source":["metrics = {}\n","# loop through the stages\n","for i in range(1, 4):\n","    # loop through the blocks\n","    for j in range(1, 4):\n","        # loop through the conv layers\n","        for k in range(1, 3):\n","            # define the layer name and grab the layer\n","            layer = f\"stage{i}_res_block{j}_conv{k}\"\n","\n","            # grab the appropriate layers\n","            prior_dist_layer = test_model.layers[get_layer_index(model, layer)]\n","\n","            # grab the weights and calculate the cosine similarity\n","            x = prior_dist_layer.get_weights()[0]\n","            metrics[layer] = np.round(calc_cosine_similarity(x), 2)\n","\n","np.set_printoptions(precision = 3)\n","print(np.mean(list(metrics.values())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2KaZtdMf1RnD","colab_type":"code","colab":{}},"source":["function ClickConnect(){\n","    document.querySelector('#top-toolbar > colab-connect-button').shadowRoot.querySelector('#connect').click()\n","    console.log(\"Working\");\n","}\n","setInterval(ClickConnect, 60000)\n","\n","# 29259"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2BejB6KwKjf","colab_type":"code","colab":{}},"source":["! rm -r models\n","! rm xresnet20.json\n","! rm xresnet20.png"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKTbrfjVUMCM","colab_type":"text"},"source":["# TEMP #"]},{"cell_type":"code","metadata":{"id":"w5l2hjRR4gbn","colab_type":"code","colab":{}},"source":["w = np.random.randn(64)\n","lamb = np.random.randn(64)\n","# 64 -> 64x64 -> 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbpAtumo9Lz5","colab_type":"code","colab":{}},"source":["new_w = np.repeat(w[..., np.newaxis], 64, axis = -1)\n","dist = np.sqrt(np.mean(np.square(new_w - new_w.T), axis = -1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUWBvTrP9Nuc","colab_type":"code","colab":{}},"source":["w_ = w - (1e-1 / dist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SbV3hG29Ssh","colab_type":"code","colab":{}},"source":["np.linalg.norm(w_ - w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tj4MPojtrrQK","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Reshape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bB9M0A05_ZHT","colab_type":"code","colab":{}},"source":["class Repeat(Layer):\n","    def __init__(self):\n","        super(Repeat, self).__init__()\n","    \n","    def call(self, x):\n","        return K.repeat(x, x.shape[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiOsXOU7BR9R","colab_type":"code","colab":{}},"source":["class Dist(Layer):\n","    def __init__(self):\n","        super(Dist, self).__init__()\n","    \n","    def call(self, x):\n","        return tf.math.reduce_mean(tf.math.abs(tf.math.subtract(x, tf.transpose(x, perm = [0, 2, 1]))), axis = -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cm6qhKN1C39i","colab_type":"code","colab":{}},"source":["# class Shift(Layer):\n","#     def __init__(self, lamb):\n","#         super(Shift, self).__init__()\n","#         self.lamb = lamb\n","\n","#     def call(self, x):\n","#         print(tf.divide(self.lamb, x[1]).shape)\n","#         return tf.subtract(x[0], tf.divide(self.lamb, x[1]))\n","\n","class Shift(Layer):\n","    def __init__(self, dhist, mom = 0.9):\n","        super(Shift, self).__init__()\n","        self.mom = mom\n","        self.dhist = dhist\n","\n","    def build(self, input_shape):\n","        self.kernel = self.add_weight(\"kernel\", shape = [input_shape[1][-1]])\n","\n","    def call(self, x):\n","        if tf.equal(self.dhist, tf.zeros(self.dhist.shape)):\n","            tf.compat.v1.assign(self.dhist, x[1])\n","            return tf.subtract(x[0], tf.divide(tf.math.square(self.kernel), x[1]))\n","        else:\n","            d_ = tf.math.add(tf.math.multiply(self.mom, self.dhist), tf.math.multiply(1 - self.mom, x[1]))\n","            tf.compat.v1.assign(self.dhist, x[1])\n","\n","            return tf.subtract(x[0], tf.divide(tf.math.square(self.kernel), d_))\n","    \n","    def get_config(self):\n","        return {\"mom\" : float(self.mom)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0K-tKMdEJFe","colab_type":"code","colab":{}},"source":["def rds_block(x, bs = 128):\n","    shortcut = x\n","    \n","    x = GlobalAveragePooling2D()(x)\n","    x = Repeat()(x)\n","    x = Dist()(x)\n","    x = Reshape((1, 1, x.shape[1]))(x)\n","    x = Shift(tf.Variable(initial_value = tf.zeros((bs, 1, 1, x.shape[-1])), trainable = False), mom = 0.9)([shortcut, x])\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGUCc9f--v2S","colab_type":"code","colab":{}},"source":["class SimilarityPenalizer(Regularizer):\n","    def __init__(self, lamb):\n","        super(SimilarityPenalizer, self).__init__()\n","        self.lamb = lamb\n","    \n","    def __call__(self, x):\n","        # x = tf.reshape(x, (-1, x.shape[-1]))\n","        # mu = tf.math.reduce_mean(x, axis = 0, keepdims = True)\n","\n","        # x_ = x - mu\n","        # x_ = tf.reshape(x_, (x_.shape[0], x_.shape[1], 1))\n","        # x_ = tf.repeat(x_, x_.shape[1], axis = -1)\n","\n","        # C  = (1 / x_.shape[0]) * tf.reduce_sum(tf.multiply(x_, tf.transpose(x_, perm = (0, 2, 1))), axis = 0)\n","\n","        # dist = (1 / 2) * (tf.norm(C) - tf.norm(tf.linalg.diag_part(C)))\n","\n","        # return (self.lamb * dist)\n","\n","        x = tf.reshape(x, (x.shape[-1], -1))\n","        x = x / (tf.linalg.norm(x, axis = -1, keepdims = True))\n","        cs = tf.matmul(x, tf.transpose(x))\n","        cs = cs * (1.0 - tf.eye(cs.shape[0], dtype = cs.dtype))\n","\n","        dist = tf.reduce_sum(tf.math.log(1.0 + tf.exp(10.0 * (cs - 1.0))))\n","\n","        # x = tf.reshape(x, (-1, x.shape[-1], 1))\n","        # x = tf.repeat(x, x.shape[1], axis = -1)\n","        # x = tf.math.l2_normalize(x, axis = 0)\n","        # xt = tf.transpose(x, perm = (0, 2, 1))\n","\n","        # cs = tf.reduce_sum(tf.multiply(x, xt), axis = 0)\n","        # cs = tf.subtract(cs, tf.eye(cs.shape[0], dtype = \"double\"))\n","\n","        # dist = tf.reduce_sum(tf.math.log(tf.add(tf.constant(1, dtype = tf.float64), tf.exp(tf.constant(10, dtype = tf.float64) * (tf.subtract(cs, 1))))))\n","\n","        return self.lamb * dist\n","    \n","    def get_config(self):\n","        return {\"lamb\" : float(self.lamb)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyKNTJrF4yuW","colab_type":"code","colab":{}},"source":["class NegReg(Regularizer):\n","    def __init__(self, reg, eta):\n","        super(NegReg, self).__init__()\n","        self.reg = reg\n","        self.eta = eta\n","    \n","    def __call__(self, x):\n","        x = tf.reshape(x, (x.shape[-1], -1))\n","        x = x / tf.linalg.norm(x, axis = -1, keepdims = True)\n","        cs = tf.matmul(x, tf.transpose(x))\n","        cs = cs - self.eta * (tf.eye(cs.shape[0], dtype = cs.dtype) - tf.ones_like(cs, dtype = cs.dtype))\n","\n","        cs = cs * (1 - tf.eye(cs.shape[0], dtype = cs.dtype))\n","        dist = tf.linalg.norm(cs)\n","\n","        return self.reg * dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEUfSoM4SY0_","colab_type":"code","colab":{}},"source":["x = np.random.randn(3, 3, 16, 64)\n","x = tf.reshape(x, (x.shape[-1], -1))\n","x = x / (tf.linalg.norm(x, axis = -1, keepdims = True))\n","cs = tf.matmul(x, tf.transpose(x))\n","cs = cs * (1.0 - tf.eye(cs.shape[0], dtype = cs.dtype))\n","\n","dist = tf.reduce_sum(tf.math.log(1.0 + tf.exp(10.0 * (cs - 1.0))))\n","\n","print(dist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qtKWZkjRv7V","colab_type":"code","colab":{}},"source":["class ActivityRegularizer(Regularizer):\n","    def __init__(self, lamb):\n","        super(ActivityRegularizer, self).__init__()\n","        self.lamb = lamb\n","    \n","    def __call__(self, x):\n","        x_ = tf.reduce_mean(x, axis = (1, 2))\n","        x_ = tf.reshape(x_, (-1, x_.shape[1], 1))\n","        x_ = tf.repeat(x_, x_.shape[-2], axis = -1)\n","\n","        dist = tf.reduce_mean(tf.abs(tf.subtract(x_, tf.transpose(x_, perm = (0, 2, 1)))))\n","\n","        return (self.lamb / dist)\n","\n","    def get_config(self):\n","        return {\"lamb\" : self.lamb}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OamS6OVZELmG","colab_type":"code","colab":{}},"source":["class Downsample(Layer):\n","    def __init__(self, filt_size = 3, stride = 2, pad_off = 0, **kwargs):\n","        super(Downsample, self).__init__(**kwargs)\n","        self.filt_size = filt_size\n","        self.stride = stride\n","        self.pad_off = pad_off\n","\n","        # pad sizes: LEFT, RIGHT, TOP, BOTTOM\n","        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2)), int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]\n","        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n","\n","    def build(self, input_shape):\n","        # initialize the appropriate blur kernel\n","        if(self.filt_size == 1):\n","            a = np.array([1.,])\n","        elif(self.filt_size == 2):\n","            a = np.array([1., 1.])\n","        elif(self.filt_size == 3):\n","            a = np.array([1., 2., 1.])\n","        elif(self.filt_size == 4):    \n","            a = np.array([1., 3., 3., 1.])\n","        elif(self.filt_size == 5):    \n","            a = np.array([1., 4., 6., 4., 1.])\n","        elif(self.filt_size == 6):    \n","            a = np.array([1., 5., 10., 10., 5., 1.])\n","        elif(self.filt_size == 7):\n","            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n","        \n","        # compute the outer product to get the final filter\n","        filt = np.outer(a, a)\n","        filt = filt / np.sum(filt)\n","\n","        # kernel shape\n","        kernel_shape = (self.filt_size, self.filt_size, input_shape[3], 1)\n","\n","        # reshape the filter into the appropriate shape and create the initializer\n","        filt = np.repeat(filt, input_shape[3])\n","        filt = np.reshape(filt, kernel_shape)\n","        blur_init = keras.initializers.constant(filt)\n","\n","        # create the blur kernel\n","        self.kernel = self.add_weight(\"kernel\", shape = kernel_shape, initializer = blur_init, trainable = False)\n","\n","        # call the parent class constructor\n","        super(Downsample, self).build(input_shape)\n","    \n","    def call(self, x):\n","        if self.filt_size == 1:\n","            if self.pad_off == 0:\n","                return x[:, ::self.stride, ::self.stride, :]\n","            else:\n","                x = tf.pad(x, paddings = tf.constant([[0, 0], self.pad_sizes[2:], self.pad_sizes[:2], [0, 0]]), mode = \"REFLECT\")\n","                return x[:, ::self.stride, ::self.stride, :]\n","        else:\n","            # pad the input (reflect pad)\n","            x = tf.pad(x, paddings = tf.constant([[0, 0], self.pad_sizes[2:], self.pad_sizes[:2], [0, 0]]), mode = \"REFLECT\")\n","            return K.depthwise_conv2d(x, self.kernel, strides = (self.stride, self.stride))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dayWPeKmGkcG","colab_type":"code","colab":{}},"source":["# initialize the dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# split the dataset into the train and validation splits\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n","                                                  test_size = 0.1, random_state = 42,\n","                                                  stratify = y_train)\n","\n","x_train = x_train.astype(\"float\") / 255.0\n","y_train = LabelBinarizer().fit_transform(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqOIdOdiFBSe","colab_type":"code","colab":{}},"source":["inpt = Input(shape = (32, 32, 3))\n","x = Conv2D(32, (3, 3), padding = \"same\")(inpt)\n","x = MaxPooling2D(pool_size = (3, 3), strides = (1, 1))(x)\n","x = Downsample(filt_size = 3, stride = 3)(x)\n","# x = rds_block(x)\n","x = Activation(\"relu\")(x)\n","x = Flatten()(x)\n","x = Dense(10, activation = \"softmax\")(x)\n","\n","temp_model = Model(inputs = inpt, outputs = x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQtjcOWjHWL5","colab_type":"code","colab":{}},"source":["temp_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qr7Yu7nsFQ7g","colab_type":"code","colab":{}},"source":["temp_model.compile(loss = \"categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFWdIPTXGvOl","colab_type":"code","colab":{}},"source":["temp_model.fit(x_train, y_train, epochs = 5, batch_size = 128)"],"execution_count":null,"outputs":[]}]}