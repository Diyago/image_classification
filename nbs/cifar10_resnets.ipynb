{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifJ7hMp3d3YX"
   },
   "source": [
    "# IMPORTS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37tSkrbPCKXO"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import multiply\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Permute\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import add\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.regularizers import l2, Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE8t_m3EOJ8a"
   },
   "outputs": [],
   "source": [
    "! mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jw1x7KRVYVVp"
   },
   "source": [
    "# CUSTOM LAYERS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ME3U9iTGFTQ"
   },
   "outputs": [],
   "source": [
    "class Mish(Layer):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X_input = Input(input_shape)\n",
    "        >>> X = Mish()(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Mish, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * K.tanh(K.softplus(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mish, self).get_config()\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYMCWMWSbPhV"
   },
   "source": [
    "# PREPROCESSORS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VssTojo5bRfr"
   },
   "outputs": [],
   "source": [
    "class PadPreprocessor:\n",
    "    def __init__(self, pad):\n",
    "        # initialize the instance variables\n",
    "        self.pad = pad\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # return the padded image\n",
    "        return np.pad(img, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVa68bIl9iDH"
   },
   "outputs": [],
   "source": [
    "class ReflectionPadPreprocessor:\n",
    "    def __init__(self, pad):\n",
    "        # initialize the instance variables\n",
    "        self.pad = pad\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # zero pad the image\n",
    "        img = np.pad(img, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)))\n",
    "\n",
    "        # reflect pad the image\n",
    "        for i, j in zip(range(self.pad), range(self.pad)):\n",
    "            xstart = self.pad\n",
    "            xend = img.shape[1] - self.pad - 1\n",
    "            ystart = self.pad\n",
    "            yend = img.shape[0] - self.pad - 1\n",
    "\n",
    "            img[:, xstart - i - 1] = img[:, xstart + i + 1]\n",
    "            img[:, xend + i + 1] = img[:, xend - i - 1]\n",
    "            img[ystart - j - 1, :] = img[ystart + j + 1, :]         \n",
    "            img[yend + j + 1, :] = img[yend - j - 1, :]   \n",
    "        \n",
    "        # return the processed image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WF6QyWvDqPWm"
   },
   "outputs": [],
   "source": [
    "class FlipPreprocessor:\n",
    "    def __init__(self, prob):\n",
    "        # initialize the instance variables \n",
    "        self.prob = prob\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        p = np.random.uniform(size = (1,))\n",
    "\n",
    "        # check to see if the image is to be flipped\n",
    "        if p <= self.prob:\n",
    "            img = cv2.flip(img, 1)\n",
    "        \n",
    "        # return the processed image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BanfTB8e8e2"
   },
   "outputs": [],
   "source": [
    "class PatchPreprocessor:\n",
    "    def __init__(self, height, width):\n",
    "        # initialize the instance variables - target height and width\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # extract a random crop from the image and return it\n",
    "        return extract_patches_2d(img, (self.height, self.width), max_patches = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9mC3MMdenCD"
   },
   "outputs": [],
   "source": [
    "class MeanPreprocessor:\n",
    "    def __init__(self, mean, std, normalize = True):\n",
    "        # initialize the instance variables\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # if the image is to be normalized, normalize it\n",
    "        if self.normalize:\n",
    "            img = img.astype(\"float\") / 255.0\n",
    "\n",
    "        # return the processed image\n",
    "        return ((img - self.mean) / self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ajD8pbeeBpn"
   },
   "outputs": [],
   "source": [
    "class ImageToArrayPreprocessor:\n",
    "    def __init__(self, data_format = None):\n",
    "        # initialize the instance variables\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        # apply the keras utility function that correctly rearranges the dimensions of the image\n",
    "        return img_to_array(img, data_format = self.data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfDTqx65eGz_"
   },
   "source": [
    "# DATA GENERATOR #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0EUKEZVeJaN"
   },
   "outputs": [],
   "source": [
    "class CifarGenerator:\n",
    "    def __init__(self, x_train, y_train, batch_size, preprocessors = None, aug = None):\n",
    "        # initialize the cifar data\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # initialize the instance variables\n",
    "        self.bs = batch_size\n",
    "        self.preprocessors = preprocessors\n",
    "        self.aug = aug\n",
    "        self.num_images = self.x_train.shape[0]\n",
    "        self.lb = LabelBinarizer()\n",
    "        self.lb.fit(y_train)\n",
    "    \n",
    "    def generator(self, passes = np.inf):\n",
    "        # initialize a variable to keep a count on the epochs\n",
    "        epochs = 0\n",
    "\n",
    "        # loop through the dataset indefinitely\n",
    "        while(epochs < passes):\n",
    "            # initialize the indices\n",
    "            indices = list(range(self.num_images))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # loop through the dataset in batches\n",
    "            for i in range(0, self.num_images, self.bs):\n",
    "                # extract the current indices\n",
    "                cur_indices = sorted(indices[i : i + self.bs])\n",
    "\n",
    "                # grab the current batch\n",
    "                x, y = self.x_train[cur_indices], self.y_train[cur_indices]\n",
    "\n",
    "                # if any preprocessors are supplied, apply them\n",
    "                if self.preprocessors is not None:\n",
    "                    # loop through the images\n",
    "                    proc_x = []\n",
    "                    for img in x:\n",
    "                        # loop through the preprocessors\n",
    "                        for p in self.preprocessors:\n",
    "                            img = p.preprocess(img)\n",
    "\n",
    "                        proc_x.append(img)\n",
    "                \n",
    "                    # update the images\n",
    "                    x = np.array(proc_x)\n",
    "                \n",
    "                # preprocess the labels\n",
    "                y = self.lb.transform(y)\n",
    "\n",
    "                # if any augmentation is supplied, apply it\n",
    "                if self.aug is not None:\n",
    "                    x, y = next(self.aug.flow(x, y, batch_size = bs))\n",
    "                \n",
    "                # yield the current batch\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iBBdXoncZIp"
   },
   "outputs": [],
   "source": [
    "class MixUpCifarGenerator:\n",
    "    def __init__(self, x_train, y_train, batch_size, alpha = 0.4, preprocessors = None, aug = None):\n",
    "        # initialize the cifar data\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # initialize the instance variables\n",
    "        self.bs = batch_size\n",
    "        self.preprocessors = preprocessors\n",
    "        self.aug = aug\n",
    "        self.alpha = alpha\n",
    "        self.num_images = self.x_train.shape[0]\n",
    "        self.lb = LabelBinarizer()\n",
    "        self.lb.fit(y_train)\n",
    "    \n",
    "    def generator(self, passes = np.inf):\n",
    "        # initialize a variable to keep a count on the epochs\n",
    "        epochs = 0\n",
    "\n",
    "        # loop through the dataset indefinitely\n",
    "        while(epochs < passes):\n",
    "            # initialize the indices\n",
    "            indices = list(range(self.num_images))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # loop through the dataset in batches\n",
    "            for i in range(0, self.num_images, self.bs):\n",
    "                # extract the current indices\n",
    "                cur_indices = sorted(indices[i : i + self.bs])\n",
    "\n",
    "                # initialize the other batch of indices\n",
    "                if i + self.bs < self.num_images:\n",
    "                    oth_indices = list(range(i, i + self.bs))\n",
    "                else:\n",
    "                    oth_indices = list(range(i, self.num_images))\n",
    "\n",
    "                # grab the data batches\n",
    "                x1, y = self.x_train[cur_indices], self.y_train[cur_indices]\n",
    "                x2 = self.x_train[oth_indices]\n",
    "\n",
    "                # if any preprocessors are supplied, apply them\n",
    "                if self.preprocessors is not None:\n",
    "                    # loop through the images\n",
    "                    proc_x1 = []\n",
    "                    proc_x2 = []\n",
    "                    for img1, img2 in zip(x1, x2):\n",
    "                        # loop through the preprocessors\n",
    "                        for p in self.preprocessors:\n",
    "                            img1 = p.preprocess(img1)\n",
    "                            img2 = p.preprocess(img2)\n",
    "\n",
    "                        proc_x1.append(img1)\n",
    "                        proc_x2.append(img2)\n",
    "                \n",
    "                    # update the images\n",
    "                    x1 = np.array(proc_x1)\n",
    "                    x2 = np.array(proc_x2)\n",
    "                \n",
    "                # randomly sample the lambda value from beta distribution.\n",
    "                lamb = np.random.beta(self.alpha + 1, self.alpha, x1.shape[0])\n",
    "\n",
    "                # remove possible duplicates\n",
    "                lamb = np.maximum(lamb, 1 - lamb)\n",
    "\n",
    "                # reshape the parameter to a suitable shape\n",
    "                xlamb = lamb.reshape(-1, 1, 1, 1)\n",
    "\n",
    "                # perform the mixup\n",
    "                x = (xlamb * x1) + ((1 - xlamb) * x2)\n",
    "\n",
    "                # preprocess the labels\n",
    "                y = self.lb.transform(y)\n",
    "\n",
    "                # if any augmentation is supplied, apply it\n",
    "                if self.aug is not None:\n",
    "                    x, y = next(self.aug.flow(x, y, batch_size = bs))\n",
    "                \n",
    "                # yield the current batch\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcEQSK1pbCyD"
   },
   "source": [
    "# MODELS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xY6fEFvoC2-D"
   },
   "outputs": [],
   "source": [
    "class XResNet:\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9, bottleneck = True, name = \"res_block\"):\n",
    "        # shortcut branch\n",
    "        shortcut = data\n",
    "\n",
    "        if bottleneck:\n",
    "            # first bottleneck block - 1x1\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Activation(\"relu\", name = name + \"_relu1\")(bn1)\n",
    "            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Activation(\"relu\", name = name + \"_relu2\")(bn2)            \n",
    "            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "            \n",
    "            # second bottleneck block - 1x1\n",
    "            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n",
    "            act3 = Activation(\"relu\", name = name + \"_relu3\")(bn3)\n",
    "            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red:\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "            \n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv3, shortcut], name = name + \"_add\")\n",
    "        \n",
    "        else:\n",
    "            # conv block 1 - 3x3\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Activation(\"relu\", name = name + \"_relu1\")(bn1)            \n",
    "            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block 2 - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Activation(\"relu\", name = name + \"_relu2\")(bn2)\n",
    "            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n",
    "                        kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red and stride != (1, 1):\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride, padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "\n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv2, shortcut], name = name + \"_add\")      \n",
    "\n",
    "        # return the addition as the output of the residual block\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n",
    "        # set the input shape\n",
    "        if K.image_data_format() == \"channels_last\":\n",
    "            input_shape = (height, width, depth)\n",
    "            chan_dim = -1\n",
    "        else:\n",
    "            input_shape = (depth, height, width)\n",
    "            chan_dim = 1\n",
    "\n",
    "        # initialize a counter to keep count of the total number of layers in the model\n",
    "        n_layers = 0\n",
    "        \n",
    "        # input block\n",
    "        inputs = Input(shape = input_shape)\n",
    "\n",
    "        # stem\n",
    "        if stem_type is \"imagenet\":\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\", \n",
    "                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv1\")(inputs)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\", \n",
    "                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv2\")(x)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\", \n",
    "                    kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv3\")(x)\n",
    "            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_max_pool\")(x)\n",
    "        elif stem_type is \"cifar\":\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\", \n",
    "                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        # modify the stages to suit bottleck\n",
    "        if bottleneck:\n",
    "            stages = [int(np.floor(st / 3)) for st in stages]\n",
    "        else:\n",
    "            stages = [int(np.floor(st / 2)) for st in stages]\n",
    "\n",
    "        # loop through the stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # set the stride value\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "\n",
    "            name = f\"stage{i + 1}_res_block1\"\n",
    "            x = XResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # loop through the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a residual module\n",
    "                name = f\"stage{i + 1}_res_block{j + 2}\"\n",
    "                x = XResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg, bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # increment the number of layers\n",
    "            if bottleneck:\n",
    "                n_layers += (3 * stages[i])\n",
    "            else:\n",
    "                n_layers += (2 * stages[i])\n",
    "        \n",
    "        # BN => RELU -> POOL\n",
    "        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n",
    "        x = Activation(\"relu\", name = \"final_relu\")(x)\n",
    "        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n",
    "        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n",
    "        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n",
    "\n",
    "        # softmax classifier\n",
    "        sc = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n",
    "        sc = Activation(\"softmax\", name = \"softmax\")(sc)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return Model(inputs = inputs, outputs = sc, name = f\"{__class__.__name__}{n_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLooEVXjGPBD"
   },
   "outputs": [],
   "source": [
    "class MXResNet:\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9,\n",
    "                        bottleneck = True, name = \"res_block\"):\n",
    "        # shortcut branch\n",
    "        shortcut = data\n",
    "\n",
    "        if bottleneck:\n",
    "            # first bottleneck block - 1x1\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Mish(name = name + \"_mish1\")(bn1)\n",
    "            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg),\n",
    "                           kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Mish(name = name + \"_mish2\")(bn2)\n",
    "            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "\n",
    "            # second bottleneck block - 1x1\n",
    "            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n",
    "            act3 = Mish(name = name + \"_mish3\")(bn3)\n",
    "            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(\n",
    "                reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red:\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride,\n",
    "                                            padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\",\n",
    "                                  kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "\n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv3, shortcut], name = name + \"_add\")\n",
    "\n",
    "        else:\n",
    "            # conv block 1 - 3x3\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Mish(name = name + \"_mish1\")(bn1)\n",
    "            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block 2 - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Mish(name = name + \"_mish2\")(bn2)\n",
    "            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red and stride != (1, 1):\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride,\n",
    "                                            padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\",\n",
    "                                  kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "\n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv2, shortcut], name = name + \"_add\")\n",
    "\n",
    "        # return the addition as the output of the residual block\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True,\n",
    "              reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n",
    "        # set the input shape\n",
    "        if K.image_data_format() == \"channels_last\":\n",
    "            input_shape = (height, width, depth)\n",
    "            chan_dim = -1\n",
    "        else:\n",
    "            input_shape = (depth, height, width)\n",
    "            chan_dim = 1\n",
    "\n",
    "        # initialize a counter to keep count of the total number of layers in the model\n",
    "        n_layers = 0\n",
    "\n",
    "        # input block\n",
    "        inputs = Input(shape = input_shape)\n",
    "\n",
    "        # stem\n",
    "        if stem_type == \"imagenet\":\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv1\")(inputs)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv2\")(x)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv3\")(x)\n",
    "            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_max_pool\")(x)\n",
    "        elif stem_type == \"cifar\":\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\",\n",
    "                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        # loop through the stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # set the stride value\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "\n",
    "            name = f\"stage{i + 1}_res_block1\"\n",
    "            x = MXResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True,\n",
    "                                         bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # loop through the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a residual module\n",
    "                name = f\"stage{i + 1}_res_block{j + 2}\"\n",
    "                x = MXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg,\n",
    "                                             bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # increment the number of layers\n",
    "            if bottleneck:\n",
    "                n_layers += (3 * stages[i])\n",
    "            else:\n",
    "                n_layers += (2 * stages[i])\n",
    "\n",
    "        # BN => RELU -> POOL\n",
    "        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n",
    "        x = Mish(name = \"final_mish\")(x)\n",
    "        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n",
    "        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n",
    "        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n",
    "\n",
    "        # softmax classifier\n",
    "        sc = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n",
    "        sc = Activation(\"softmax\", name = \"softmax\")(sc)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return Model(inputs = inputs, outputs = sc, name = f\"{__class__.__name__}{n_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQYuIOVdIZ09"
   },
   "outputs": [],
   "source": [
    "class SEMXResNet:\n",
    "    @staticmethod\n",
    "    def squeeze_excite_block(tensor, ratio = 16, name = \"se_block\"):\n",
    "        init = tensor\n",
    "        channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "        filters = init.shape[channel_axis]\n",
    "        se_shape = (1, 1, filters)\n",
    "\n",
    "        se = GlobalAveragePooling2D(name = name + \"_gap\")(init)\n",
    "        se = Reshape(se_shape, name = name + \"_reshape\")(se)\n",
    "        se = Dense(filters // ratio, kernel_initializer = 'he_normal', use_bias = False, name = name + \"_squeeze\")(se)\n",
    "        se = Activation(\"relu\", name = name + \"_squeeze_relu\")(se)\n",
    "        se = Dense(filters, kernel_initializer = 'he_normal', use_bias = False, name = name + \"_excite\")(se)\n",
    "        se = Activation(\"sigmoid\", name = name + \"_excite_sigmoid\")(se)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            se = Permute((3, 1, 2))(se)\n",
    "\n",
    "        x = multiply([init, se], name = name + \"_scale\")\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chan_dim, red = False, reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9,\n",
    "                        bottleneck = True, name = \"res_block\"):\n",
    "        # shortcut branch\n",
    "        shortcut = data\n",
    "\n",
    "        if bottleneck:\n",
    "            # first bottleneck block - 1x1\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Mish(name = name + \"_mish1\")(bn1)\n",
    "            conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg),\n",
    "                           kernel_initializer = \"he_normal\", name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Mish(name = name + \"_mish2\")(bn2)\n",
    "            conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "\n",
    "            # second bottleneck block - 1x1\n",
    "            bn3 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn3\")(conv2)\n",
    "            act3 = Mish(name = name + \"_mish3\")(bn3)\n",
    "            conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(\n",
    "                reg), kernel_initializer = \"he_normal\", name = name + \"_conv3\")(act3)\n",
    "\n",
    "            # se module\n",
    "            conv3 = SEMXResNet.squeeze_excite_block(conv3, name = name + \"_se_block\")\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red:\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride,\n",
    "                                            padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\",\n",
    "                                  kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "\n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv3, shortcut], name = name + \"_add\")\n",
    "\n",
    "        else:\n",
    "            # conv block 1 - 3x3\n",
    "            bn1 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn1\")(data)\n",
    "            act1 = Mish(name = name + \"_mish1\")(bn1)\n",
    "            conv1 = Conv2D(K, (3, 3), strides = stride, padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv1\")(act1)\n",
    "\n",
    "            # conv block 2 - 3x3\n",
    "            bn2 = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = name + \"_bn2\")(conv1)\n",
    "            act2 = Mish(name = name + \"_mish2\")(bn2)\n",
    "            conv2 = Conv2D(K, (3, 3), padding = \"same\", use_bias = False,\n",
    "                           kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = name + \"_conv2\")(act2)\n",
    "\n",
    "            # se module\n",
    "            conv2 = SEMXResNet.squeeze_excite_block(conv2, name = name + \"_se_block\")\n",
    "\n",
    "            # if dimensions are to be reduced, apply a conv layer to the shortcut\n",
    "            if red and stride != (1, 1):\n",
    "                shortcut = AveragePooling2D(pool_size = (2, 2), strides = stride,\n",
    "                                            padding = \"same\", name = name + \"_avg_pool\")(act1)\n",
    "                shortcut = Conv2D(K, (1, 1), strides = (1, 1), use_bias = False, kernel_initializer = \"he_normal\",\n",
    "                                  kernel_regularizer = l2(reg), name = name + \"_red\")(shortcut)\n",
    "                shortcut = BatchNormalization(name = name + \"_red_bn\")(shortcut)\n",
    "\n",
    "            # add the shortcut and final conv\n",
    "            x = add([conv2, shortcut], name = name + \"_add\")\n",
    "\n",
    "        # return the addition as the output of the residual block\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(height, width, depth, classes, stages, filters, stem_type = \"imagenet\", bottleneck = True,\n",
    "              reg = 1e-4, bn_eps = 2e-5, bn_mom = 0.9):\n",
    "        # set the input shape\n",
    "        if K.image_data_format() == \"channels_last\":\n",
    "            input_shape = (height, width, depth)\n",
    "            chan_dim = -1\n",
    "        else:\n",
    "            input_shape = (depth, height, width)\n",
    "            chan_dim = 1\n",
    "\n",
    "        # initialize a counter to keep count of the total number of layers in the model\n",
    "        n_layers = 0\n",
    "\n",
    "        # input block\n",
    "        inputs = Input(shape = input_shape)\n",
    "\n",
    "        # stem\n",
    "        if stem_type == \"imagenet\":\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (2, 2), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv1\")(inputs)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv2\")(x)\n",
    "            x = Conv2D(filters[0], (3, 3), strides = (1, 1), use_bias = False, padding = \"same\",\n",
    "                       kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"stem_conv3\")(x)\n",
    "            x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\", name = \"stem_max_pool\")(x)\n",
    "        elif stem_type == \"cifar\":\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = \"same\", kernel_initializer = \"he_normal\",\n",
    "                       kernel_regularizer = l2(reg), name = \"stem_conv\")(inputs)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        # loop through the stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # set the stride value\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "\n",
    "            name = f\"stage{i + 1}_res_block1\"\n",
    "            x = SEMXResNet.residual_module(x, filters[i + 1], stride, chan_dim, reg = reg, red = True,\n",
    "                                           bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # loop through the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a residual module\n",
    "                name = f\"stage{i + 1}_res_block{j + 2}\"\n",
    "                x = SEMXResNet.residual_module(x, filters[i + 1], (1, 1), chan_dim, reg = reg,\n",
    "                                               bn_eps = bn_eps, bn_mom = bn_mom, bottleneck = bottleneck, name = name)\n",
    "\n",
    "            # increment the number of layers\n",
    "            if bottleneck:\n",
    "                n_layers += (3 * stages[i])\n",
    "            else:\n",
    "                n_layers += (2 * stages[i])\n",
    "\n",
    "        # BN => RELU -> POOL\n",
    "        x = BatchNormalization(axis = chan_dim, epsilon = bn_eps, momentum = bn_mom, name = \"final_bn\")(x)\n",
    "        x = Mish(name = \"final_mish\")(x)\n",
    "        x1 = GlobalAveragePooling2D(name = \"global_avg_pooling\")(x)\n",
    "        x2 = GlobalMaxPooling2D(name = \"global_max_pooling\")(x)\n",
    "        x = concatenate([x1, x2], axis = -1, name = \"concatenate\")\n",
    "\n",
    "        # softmax classifier\n",
    "        sc = Dense(classes, kernel_initializer = \"he_normal\", kernel_regularizer = l2(reg), name = \"classifier\")(x)\n",
    "        sc = Activation(\"softmax\", name = \"softmax\")(sc)\n",
    "\n",
    "        # increment the number of layers\n",
    "        n_layers += 1\n",
    "\n",
    "        print(f\"[INFO] {__class__.__name__}{n_layers} built successfully!\")\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return Model(inputs = inputs, outputs = sc, name = f\"{__class__.__name__}{n_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2026,
     "status": "ok",
     "timestamp": 1593929389134,
     "user": {
      "displayName": "Varun Anand",
      "photoUrl": "",
      "userId": "08614398225445890360"
     },
     "user_tz": -330
    },
    "id": "-aIcKqUyEHKW",
    "outputId": "3d92b0aa-2ee0-4a46-b6a1-5cabc8160319"
   },
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"xresnet20\" : XResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n",
    "    # \"mxresnet20\" : MXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n",
    "    # \"se-mxresnet20\" : SEMXResNet.build(32, 32, 3, 10, [6, 6, 6], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False, reg = 1e-4)\n",
    "    # \"xresnet44\" : XResNet.build(32, 32, 3, 10, [14, 14, 14], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False),\n",
    "    # \"xresnet56\" : XResNet.build(32, 32, 3, 10, [18, 18, 18], [16, 16, 32, 64], stem_type = \"cifar\", bottleneck = False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW-Kq4s0sCj8"
   },
   "source": [
    "# CALLBACKS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-58a06NsE55"
   },
   "outputs": [],
   "source": [
    "class TrainingMonitor(BaseLogger):\n",
    "    def __init__(self, fig_path, json_path = None, start_at = 0):\n",
    "        # store the output path for the figure, the path to the JSON serialized file, and the starting epoch\n",
    "        super(TrainingMonitor, self).__init__()\n",
    "        self.fig_path = fig_path\n",
    "        self.json_path = json_path\n",
    "        self.start_at = start_at\n",
    "\n",
    "    def on_train_begin(self, logs = {}):\n",
    "        # initialize the history dictionary\n",
    "        self.H = {}\n",
    "\n",
    "        # if the JSON history path exists, load the training history\n",
    "        if self.json_path is not None:\n",
    "            if os.path.exists(self.json_path):\n",
    "                self.H = json.loads(open(self.json_path).read())\n",
    "\n",
    "                # check to see if a starting epoch was supplied\n",
    "                if self.start_at > 0:\n",
    "                    # loop over the entries in the history log and trim any entries that are past the starting epoch\n",
    "                    for key in self.H.keys():\n",
    "                        self.H[key] = self.H[key][:self.start_at]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        # loop over the logs and update the loss, accuracy etc, for the entire training process\n",
    "        for (k, v) in logs.items():\n",
    "            l = self.H.get(k, [])\n",
    "            l.append(v)\n",
    "            self.H[k] = l\n",
    "\n",
    "        # check to see if the training history should be serialized to file\n",
    "        if self.json_path is not None:\n",
    "            f = open(self.json_path, \"w\")\n",
    "            f.write(json.dumps(self.H))\n",
    "            f.close()\n",
    "\n",
    "        # ensure atleast two epochs have passed before plotting\n",
    "        if len(self.H[\"loss\"]) > 1:\n",
    "            # plot the training loss and accuracy\n",
    "            N = np.arange(0, len(self.H[\"loss\"]))\n",
    "            plt.figure()\n",
    "            plt.style.use(\"ggplot\")\n",
    "            plt.plot(N, self.H[\"loss\"], label = \"train_loss\")\n",
    "            plt.plot(N, self.H[\"val_loss\"], label = \"val_loss\")\n",
    "            plt.plot(N, self.H[\"accuracy\"], label = \"acc\")\n",
    "            plt.plot(N, self.H[\"val_accuracy\"], label = \"val_acc\")\n",
    "            plt.title(\"Training Loss [Epoch {}]\".format(len(self.H[\"loss\"])))\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "            # save the figure\n",
    "            plt.savefig(self.fig_path)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyEkFtoGr_7f"
   },
   "source": [
    "# TRAINING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NPeYUDos98I"
   },
   "outputs": [],
   "source": [
    "# initialize the dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# split the dataset into the train and validation splits\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size = 0.1, random_state = 42,\n",
    "                                                  stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCsQhhpWtrEY"
   },
   "outputs": [],
   "source": [
    "# define training constants\n",
    "epochs = 180\n",
    "bs = 128\n",
    "steps_per_epoch = np.ceil(x_train.shape[0] / bs)\n",
    "validation_steps = np.ceil(x_val.shape[0] / bs)\n",
    "test_steps = np.ceil(x_test.shape[0] / bs)\n",
    "model_name = \"xresnet20\"\n",
    "init_lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_w3A34Qs31r"
   },
   "outputs": [],
   "source": [
    "# initialize the preprocessors\n",
    "# rpp = ReflectionPadPreprocessor(4)\n",
    "pp = PadPreprocessor(4)\n",
    "fp = FlipPreprocessor(0.5)\n",
    "patchp = PatchPreprocessor(32, 32)\n",
    "mp = MeanPreprocessor([0.4914, 0.4822, 0.4465], [0.247, 0.2435, 0.2616])\n",
    "iap = ImageToArrayPreprocessor()\n",
    "\n",
    "# initialize the data generators\n",
    "train_datagen = CifarGenerator(x_train, y_train, bs, preprocessors = [pp, fp, patchp, mp, iap]).generator()\n",
    "val_datagen = CifarGenerator(x_val, y_val, bs, preprocessors = [mp, iap]).generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1LNyRTfrI1h"
   },
   "outputs": [],
   "source": [
    "# step decay learning rate scheduler\n",
    "def lr_sched(epoch):\n",
    "    lr = init_lr\n",
    "\n",
    "    if epoch < 1:\n",
    "        lr = init_lr / 10\n",
    "    elif epoch < 90:\n",
    "        lr = init_lr\n",
    "    elif epoch < 135:\n",
    "        lr = init_lr / 10\n",
    "    else:\n",
    "        lr = init_lr / 100\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# # cosine decay learning rate scheduler\n",
    "# class CosineScheduler(Callback):\n",
    "#     def __init__(self, max_lr, steps_per_epoch, tot_epochs, warmup = 5):\n",
    "#         # parent class constructor\n",
    "#         super(CosineScheduler, self).__init__()\n",
    "\n",
    "#         # initialize the instance variables\n",
    "#         self.max_lr = max_lr\n",
    "#         self.warm_steps = steps_per_epoch * warmup\n",
    "#         self.reg_steps = steps_per_epoch * (tot_epochs - warmup)\n",
    "#         self.history = {\"lrs\" : []}\n",
    "    \n",
    "#     def on_train_begin(self, logs = None):\n",
    "#         # initialize a counter to keep track of the number of batches seen\n",
    "#         self.iterations = 0\n",
    "    \n",
    "#     def on_batch_begin(self, batch, logs = None):\n",
    "#         # increment the number of iterations\n",
    "#         self.iterations += 1\n",
    "\n",
    "#         # calculate the learning rate\n",
    "#         if self.iterations <= self.warm_steps:\n",
    "#             lr = (self.iterations / self.warm_steps) * self.max_lr\n",
    "#         else:\n",
    "#             lr = (self.max_lr / 2.0) * (1 + np.cos(((self.iterations - self.warm_steps) / self.reg_steps) * np.pi))\n",
    "        \n",
    "#         # update the learning rate\n",
    "#         K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "#         # add the current learning rate to the history dictionary\n",
    "#         self.history[\"lrs\"].append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvray3Prsi6P"
   },
   "outputs": [],
   "source": [
    "# initialize the callbacks\n",
    "mc = ModelCheckpoint(os.path.sep.join([\"models\", model_name + \"_{epoch:03d}.h5\"]))\n",
    "tm = TrainingMonitor(f\"{model_name}.png\", f\"{model_name}.json\")\n",
    "lr = LearningRateScheduler(lr_sched)\n",
    "# cs = CosineScheduler(init_lr, steps_per_epoch, epochs)\n",
    "callbacks = [mc, tm, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nStBph8FnJJ3"
   },
   "outputs": [],
   "source": [
    "# initialize the model and compile it\n",
    "model = MODELS[model_name]\n",
    "opt = SGD(lr = init_lr, momentum = 0.9)\n",
    "# loss = CategoricalCrossentropy(label_smoothing = 0.1)\n",
    "loss = CategoricalCrossentropy()\n",
    "model.compile(optimizer = opt, loss = loss, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 89246,
     "status": "ok",
     "timestamp": 1592925314450,
     "user": {
      "displayName": "Varun Anand",
      "photoUrl": "",
      "userId": "08614398225445890360"
     },
     "user_tz": -330
    },
    "id": "nN67f8DPvW5t",
    "outputId": "d07f5e28-7bee-4076-dd03-c6d482b27289",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit_generator(train_datagen, steps_per_epoch = steps_per_epoch, epochs = epochs,\n",
    "                    validation_data = val_datagen, validation_steps = validation_steps,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mR2W2_lC0fC"
   },
   "source": [
    "# INFERENCE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15354,
     "status": "ok",
     "timestamp": 1592925941480,
     "user": {
      "displayName": "Varun Anand",
      "photoUrl": "",
      "userId": "08614398225445890360"
     },
     "user_tz": -330
    },
    "id": "8fQggWD1C2Nt",
    "outputId": "ea0c8d1f-ba85-432b-b994-e530656b4677"
   },
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "test_model = load_model(f\"models/{model_name}_{epochs}.h5\", custom_objects = {\"Downsample\" : Downsample, \"Mish\" : Mish})\n",
    "\n",
    "# initialize the data generator\n",
    "test_gen = CifarGenerator(x_test, y_test, bs, preprocessors = [mp, iap]).generator()\n",
    "\n",
    "# evaluate the model\n",
    "H = test_model.evaluate_generator(test_gen, steps = test_steps)\n",
    "print(H)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOwicmsWTsZqDV8gWGzCdpC",
   "collapsed_sections": [
    "ifJ7hMp3d3YX",
    "jw1x7KRVYVVp",
    "pYMCWMWSbPhV",
    "kfDTqx65eGz_",
    "NcEQSK1pbCyD",
    "UW-Kq4s0sCj8",
    "aKTbrfjVUMCM"
   ],
   "name": "cifar10_resnets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
